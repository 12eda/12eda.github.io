[{"id":"b1dc64af12759d9df8f8c0d6049f8b08","title":"DFMB-data_utils","content":"涉及以下函数的实现：\n12__all__ = [&#x27;get_pid_list&#x27;, &#x27;get_go_list&#x27;, &#x27;get_pid_go&#x27;, &#x27;get_pid_go_sc&#x27;, &#x27;get_data&#x27;, &#x27;output_res&#x27;, &#x27;get_mlb&#x27;,           &#x27;get_pid_go_mat&#x27;, &#x27;get_pid_go_sc_mat&#x27;, &#x27;get_ppi_idx&#x27;, &#x27;get_homo_ppi_idx&#x27;]\n\n\n\n123456789#既可以处理文件输入，也可以直接处理列表输入。def get_pid_list(pid_list_file):    try:        #遍历文件的每一行，对每一行使用 split() 方法分割，取分割后的第一个元素 [0]，将所有行的第一个元素组成列表返回。        with open(pid_list_file) as fp:            return [line.split()[0] for line in fp]    except TypeError:        return pid_list_file    #最终返回蛋白质ID列表 。\n\n\n123456789#为输入的每个PID返回其对应的所有GO术语列表，如果PID在文件中不存在则返回空列表。#返回我们需要的PID对应的GO术语def get_go_list(pid_go_file, pid_list):    pid_go = defaultdict(list)#defaultdict的好处是：当访问不存在的键时会自动创建空列表    with open(pid_go_file) as fp:        for line in fp:            line_list=line.split()            pid_go[(line_list)[0]].append(line_list[1])    return [pid_go[pid_] for pid_ in pid_list]\n\n\n1234567891011#返回文件中所有PID与GO术语的完整映射关系def get_pid_go(pid_go_file):    if pid_go_file is not None:        pid_go = defaultdict(list)        with open(pid_go_file) as fp:            for line in fp:                line_list=line.split()                pid_go[(line_list)[0]].append(line_list[1])        return dict(pid_go)    else:        return None\n\n123456789#获取带得分的PID-GO映射def get_pid_go_sc(pid_go_sc_file):    pid_go_sc = defaultdict(dict)    with open(pid_go_sc_file) as fp:        for line in fp:            line_list=line.split()            pid_go_sc[line_list[0]][line_list[1]] = float((line_list)[2])            #嵌套字典结构 exp：&#x27;P67890&#x27;: &#123;&#x27;GO:0003674&#x27;: 0.92&#125;    return dict(pid_go_sc)\n\n\n12345#返回指定PID列表对应的ESM向量def get_esm_list(pid_esm_file,pid_list):    with open(pid_esm_file,&#x27;rb&#x27;) as fr:        pid_esm=pkl.load(fr)    return [pid_esm[pid_] for pid_ in pid_list]\n\n\n12345678#整合三个不同数据源的信息，为每个蛋白质ID提供完整的数据#从FASTA文件中（里面是蛋白质的名称、描述和氨基酸序列），提取的所有蛋白质ID,随后获取他们的GO术语列表,ESM向量列表def get_data(fasta_file, pid_go_file,pid_esm_file):    pid_list = []    for seq in SeqIO.parse(fasta_file, &#x27;fasta&#x27;):        pid_list.append(seq.id)        return pid_list, get_go_list(pid_go_file, pid_list), get_esm_list(pid_esm_file,pid_list)\n\n\n123456789#将GO术语列表转换为二进制矩阵，像是一个多标签的独热编码def get_mlb(mlb_path: Path, labels=None, **kwargs) -&gt; MultiLabelBinarizer:    if mlb_path.exists():#如果缓存文件存在，直接加载并返回已训练的二值化器        return joblib.load(mlb_path)    #将多标签（每个蛋白质有多个GO术语）转换为二进制矩阵，使用了稀疏矩阵。    mlb = MultiLabelBinarizer(sparse_output=True, **kwargs)    mlb.fit(labels)    joblib.dump(mlb, mlb_path)    return mlb\n\n12345678#把预测结果保存到文件里def output_res(res_path: Path, pid_list, go_list, sc_mat):    res_path.parent.mkdir(parents=True, exist_ok=True)    with open(res_path, &#x27;w&#x27;) as fp:        for pid_, sc_ in zip(pid_list, sc_mat):            for go_, s_ in zip(go_list, sc_):                if s_ &gt; 0.0:                    print(pid_, go_, s_, sep=&#x27;\\t&#x27;, file=fp)\n\n\n123456789101112#创建一个稀疏矩阵来表示蛋白质和GO术语之间的关系def get_pid_go_mat(pid_go, pid_list, go_list):    go_mapping = &#123;go_: i for i, go_ in enumerate(go_list)&#125;    r_, c_, d_ = [], [], []    for i, pid_ in enumerate(pid_list):        if pid_ in pid_go:            for go_ in pid_go[pid_]:                if go_ in go_mapping:                    r_.append(i)                    c_.append(go_mapping[go_])                    d_.append(1)    return ssp.csr_matrix((d_, (r_, c_)), shape=(len(pid_list), len(go_list)))\n\n\n123456789#创建一个得分矩阵，记录每个蛋白质在每个GO术语上的预测得分，如果没有得分，就填入一个极小的数-1e100def get_pid_go_sc_mat(pid_go_sc, pid_list, go_list):    sc_mat = np.zeros((len(pid_list), len(go_list)))    #enumerate() 给每个元素加上了编号    for i, pid_ in enumerate(pid_list):        if pid_ in pid_go_sc:            for j, go_ in enumerate(go_list):                sc_mat[i, j] = pid_go_sc[pid_].get(go_, -1e100)    return sc_mat\n\n\n123456789101112131415161718192021#从输入的pid_list中筛选出那些存在于net_pid_map中的蛋白质，# 然后返回这些蛋白质的索引、ID、网络ID、标签（如果data_y不为None）和ESM特征。def get_ppi_idx(pid_list, data_y, net_pid_map, data_esm):#data_y 是一个二维数组，表示蛋白质和GO术语之间的关系    pid_list_ = tuple(zip(*[(i, pid, net_pid_map[pid]) for i, pid in enumerate(pid_list) if pid in net_pid_map]))    #遍历 pid_list 中的每个蛋白质，同时获得索引 i 和蛋白质ID pid    #只保留那些存在于 net_pid_map 中的蛋白质    #对于每个保留的蛋白质，创建一个三元组 (i, pid, net_pid_map[pid])    #zip(*...) 是一个&quot;转置&quot;操作，它把数据从&quot;行格式&quot;转换成&quot;列格式&quot;。    #在转化成元组，也就是索引放在一起，ID放在一起，所有网络ID也放在一起。    assert pid_list_    pid_list_ = (np.asarray(pid_list_[0]), pid_list_[1], np.asarray(pid_list_[2]))        if data_esm is None:        esm_list=None    else:        esm_list=[]        for i in pid_list_[0]:            esm_list.append(data_esm[i])    #从原始数据中提取出存在于PPI网络中的蛋白质的完整信息包，包括索引、ID、网络ID、标签和特征向量。    return pid_list_[0], pid_list_[1], pid_list_[2], data_y[pid_list_[0]] if data_y is not None else data_y,esm_list\n\n\n123456789101112#处理不在PPI网络中的蛋白质，通过同源比对找到最相似的蛋白质来进行映射。def get_homo_ppi_idx(pid_list, fasta_file, data_y, net_pid_map, data_esm, net_blastdb, blast_output_path):    blast_sim = blast(net_blastdb, pid_list, fasta_file, blast_output_path)    #包含了每个查询蛋白质与PPI网络中每个相关蛋白质的相似性比较结果。    &#x27;&#x27;&#x27;    blast_sim: dict, blast_sim[query_pid]-&gt;&#123;protein1: similarity1, protein2: similarity2, ...&#125;    &#x27;&#x27;&#x27;    pid_list_ = []    for i, pid in enumerate(pid_list):        blast_sim[pid][None] = float(&#x27;-inf&#x27;)        #防止BLAST结果为空时程序出错        \n\n\n1234pid_ = pid if pid in net_pid_map else max(blast_sim[pid].items(), key=lambda x: x[1])[0]        #key=lambda x: x[1]：告诉 max() 按相似度得分（元组的第二个元素）比较        #max() 函数返回一个元组，元组的第一个元素是相似度最高的蛋白质ID，第二个元素是相似度得分。        #[0]：从结果元组中提取蛋白质ID（第一个元素）\n\n当蛋白质直接在PPI网络中：\n当蛋白质不在PPI网络中，使用最相似蛋白质：\n12345if pid_ is not None:        pid_list_.append((i, pid, net_pid_map[pid_]))pid_list_ = tuple(zip(*pid_list_))#将&quot;行格式&quot;的数据转换为&quot;列格式&quot;pid_list_ = (np.asarray(pid_list_[0]), pid_list_[1], np.asarray(pid_list_[2]))#将索引和网络ID转换为数组，保持蛋白质ID为元组\n\n\n\n123456789if data_esm is None:    esm_list=Noneelse:    esm_list=[]    for i in pid_list_[0]:        esm_list.append(data_esm[i])        return pid_list_[0], pid_list_[1], pid_list_[2], data_y[pid_list_[0]] if data_y is not None else data_y,esm_list\n\n\n最终结果：\n","slug":"DFMB-data_utils","date":"2025-11-05T16:00:00.000Z","categories_index":"DeepFMB","tags_index":"项目","author_index":"Qushubiao"},{"id":"54f39c547c269df1bc57406b3c3efac2","title":"DFMB-preprocessing","content":"​\t这是一个蛋白质相互作用网络(PPI)的图数据处理脚本，它通过加载稀疏矩阵格式的PPI网络数据，为每个蛋白质节点保留权重最高的前top个连接边，添加自环并进行对称归一化处理，最终构建成有向的DGL图格式并保存，用于后续图神经网络模型的训练和预测。\n1234567891011121314151617import clickimport numpy as npimport scipy.sparse as sspimport networkx as nximport dglimport dgl.datafrom tqdm import tqdm, trangefrom logzero import logger#此时传进来的是稀疏矩阵def get_norm_net_mat(net_mat):    degree_0 = np.asarray(net_mat.sum(0)).squeeze()#统计出度,将结果转换为1维数组    mat_d_0 = ssp.diags(degree_0 ** -0.5, format=&#x27;csr&#x27;)#对每个度值求 -1/2 次方,创建稀疏对角矩阵    degree_1 = np.asarray(net_mat.sum(1)).squeeze()#统计入度,将结果转换为1维数组    mat_d_1 = ssp.diags(degree_1 ** -0.5, format=&#x27;csr&#x27;)    return mat_d_0 @ net_mat @ mat_d_1\n\nnet_mat的格式可能是，随后的操作会得到两个对角矩阵，分明是入度和出度的归一化矩阵随后进行矩阵乘法运算，得到了归一化后的邻接矩阵，这样可以使得所有边的权重被重新缩放，防止高度数节点在消息传递中占据主导地位，使矩阵更平衡。\n123456def main(ppi_net_mat_path, dgl_graph_path, top):    mat_ = ssp.load_npz(ppi_net_mat_path)# 加载原始数据，返回一个稀疏矩阵对象    ppi_net_mat = mat_ + ssp.eye(mat_.shape[0], format=&#x27;csr&#x27;)#添加自环    logger.info(F&#x27;&#123;ppi_net_mat.shape&#125; &#123;ppi_net_mat.nnz&#125;&#x27;)#记录矩阵的基本信息，输出矩阵的维度和非零元素个数    r, c, v = [], [], []# 分别存储行索引、列索引、权重值    \n\n假设有3个蛋白质，top=2：\n原始PPI网络：123节点0: [1:0.9, 2:0.8, 3:0.7]  # 3个连接节点1: [0:0.9, 2:0.5, 4:0.4]  # 3个连接  节点2: [0:0.8, 1:0.5]         # 2个连接\n\n添加自环后：123节点0: [0:1.0, 1:0.9, 2:0.8, 3:0.7]节点1: [0:0.9, 1:1.0, 2:0.5, 4:0.4]节点2: [0:0.8, 1:0.5, 2:1.0]\n\n最终三个数组的内容：123r = [0, 0, 1, 1, 2, 2]    # 源节点：0,0,1,1,2,2c = [0, 1, 1, 0, 2, 0]    # 目标节点：0,1,1,0,2,0  v = [1.0, 0.9, 1.0, 0.9, 1.0, 0.8]  # 权重\n\n1234567891011for i in trange(ppi_net_mat.shape[0]):        # 遍历每个蛋白质，按亲密程度排序，只取前top个，减少很多弱连接（噪声）        for v_, c_ in sorted(zip(ppi_net_mat[i].data, ppi_net_mat[i].indices), reverse=True)[:top]:            r.append(i) # 源节点索引            c.append(c_) # 目标节点索引            v.append(v_) # 权重值    #用新数据创建新的网络    #转置（.T）后，每个节点从关注它的邻居接收消息，节点可以聚合来自多个邻居的信息    #然后再标准化一下    ppi_net_mat = get_norm_net_mat(ssp.csc_matrix((v, (r, c)), shape=ppi_net_mat.shape).T)    logger.info(F&#x27;&#123;ppi_net_mat.shape&#125; &#123;ppi_net_mat.nnz&#125;&#x27;)\n\n123456789101112# 构建的CSC矩阵：[[1.0, 0.9, 0.0], [0.9, 1.0, 0.0], [0.8, 0.0, 1.0]]# 转置后的矩阵：[[1.0, 0.9, 0.8], [0.9, 1.0, 0.0], [0.0, 0.0, 1.0]] #归一化后矩阵 = [[0.58, 0.52, 0.46], [0.52, 0.58, 0.00], [0.00, 0.00, 0.58]]\n\nshape=ppi_net_mat.shape 保持与原矩阵相同的维度。筛选时构建的关系：节点视角的”我关注谁”，表示”节点关注这些重要的邻居”。但GNN消息传递需要：邻居视角的”谁关注我”，表示后续需要从邻居节点来获取、聚合信息，所以在这里进行转置。GNN需要的是”入边”关系，而不是”出边”关系。\n12#转化成COO格式的稀疏矩阵ppi_net_mat_coo = ssp.coo_matrix(ppi_net_mat)\n\n\n1234567891011nx_graph = nx.DiGraph()#NetworkX格式：便于图操作和调试for u, v, d in tqdm(zip(ppi_net_mat_coo.row, ppi_net_mat_coo.col, ppi_net_mat_coo.data),                    total=ppi_net_mat_coo.nnz, desc=&#x27;PPI&#x27;):    nx_graph.add_edge(u, v, ppi=d)#图神经网络专用，高效计算dgl_graph = dgl.from_networkx(nx_graph, edge_attrs=[&#x27;ppi&#x27;])#将nx_graph边的 ppi 属性转换为DGL边的特征# 确保没有蛋白质被超过top个其他蛋白质关注assert dgl_graph.in_degrees().max() &lt;= topdgl.data.utils.save_graphs(dgl_graph_path, dgl_graph)","slug":"DFMB-preprocessing","date":"2025-11-04T16:00:00.000Z","categories_index":"DeepFMB","tags_index":"项目","author_index":"Qushubiao"},{"id":"80bdf4334493e2cc51bf0382bdaa975b","title":"DFMB-generate_data","content":"以下为实验代码：\n主要用于解析基因本体(GO)数据库的层级结构，并针对蛋白质的三个功能类别（生物过程、分子功能、细胞组分）分别处理训练集和测试集数据，生成包含蛋白质ID列表、功能注释关系和蛋白质序列的标准化文件，为后续深度学习模型提供结构化的输入数据。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276import mathimport click as ckimport numpy as npimport pandas as pdimport pickle as pklimport pandas as pdimport numpy as npimport sysimport osimport gzipimport jsonfrom collections import deque, Counter,defaultdictAAS = &#123;&#x27;M&#x27;: 18, &#x27;F&#x27;: 15, &#x27;A&#x27;: 3, &#x27;P&#x27;: 9, &#x27;I&#x27;: 12, &#x27;D&#x27;: 11, &#x27;V&#x27;: 6, &#x27;K&#x27;: 7, &#x27;S&#x27;: 2, &#x27;N&#x27;: 13, &#x27;W&#x27;: 20, &#x27;T&#x27;: 10, &#x27;G&#x27;: 4, &#x27;L&#x27;: 1, &#x27;Y&#x27;: 16, &#x27;E&#x27;: 5, &#x27;H&#x27;: 17, &#x27;R&#x27;: 8, &#x27;Q&#x27;: 14, &#x27;C&#x27;: 19&#125;#将20种氨基酸的单字母代码映射到数字BIOLOGICAL_PROCESS = &#x27;GO:0008150&#x27;MOLECULAR_FUNCTION = &#x27;GO:0003674&#x27;CELLULAR_COMPONENT = &#x27;GO:0005575&#x27;FUNC_DICT = &#123;    &#x27;cc&#x27;: CELLULAR_COMPONENT,    &#x27;mf&#x27;: MOLECULAR_FUNCTION,    &#x27;bp&#x27;: BIOLOGICAL_PROCESS&#125;NAMESPACES = &#123;    &#x27;cc&#x27;: &#x27;cellular_component&#x27;,    &#x27;mf&#x27;: &#x27;molecular_function&#x27;,    &#x27;bp&#x27;: &#x27;biological_process&#x27;&#125;NAMESPACES_reverse = &#123;     &#x27;cellular_component&#x27;:&#x27;cc&#x27;,     &#x27;molecular_function&#x27;: &#x27;mf&#x27;,     &#x27;biological_process&#x27;:&#x27;bp&#x27;,&#125;#模型性能评估的四个指标evaluations = [&#x27;precision&#x27;,&#x27;recall&#x27;,&#x27;fmax&#x27;,&#x27;aupr&#x27;]class Ontology(object):    #ont 是一个字典，它存储了从obo文件中解析出来的所有术语    #ic是信息内容（Information Content）被用来衡量一个基因功能术语（GO Term）的特定性。    def __init__(self, filename=&#x27;./uniprot_sprot_train_test_data_oral/go.obo&#x27;, with_rels=False):        #obo文件：记录了所有基因功能的定义和层级关系        #with_rels=False：不加载详细的关系类型        self.ont = self.load(filename, with_rels)        self.ic = None    def has_term(self, term_id):        return term_id in self.ont    #计算每个功能的&quot;稀有度评分&quot;    #annots:蛋白质的功能注释列表    def calculate_ic(self, annots):    #annots = [    # 每个小列表代表一个蛋白质的&quot;功能简历&quot;    #[&#x27;GO:0008150&#x27;, &#x27;GO:0003674&#x27;, &#x27;GO:0005575&#x27;],  # 蛋白质1的功能    #[&#x27;GO:0008150&#x27;, &#x27;GO:0003674&#x27;],                 # 蛋白质2的功能      #[&#x27;GO:0005575&#x27;, &#x27;GO:0009987&#x27;],                 # 蛋白质3的功能    #[&#x27;GO:0008150&#x27;],                               # 蛋白质4的功能    # ... 成千上万个蛋白质    #]        cnt = Counter()        for x in annots:            cnt.update(x)        #cnt是一个键值对的合集，记录了每个GO术语出现的次数        #cnt = &#123;&#x27;GO:0008150&#x27;: 2, &#x27;GO:0003674&#x27;: 2, &#x27;GO:0005575&#x27;: 1&#125;        self.ic = &#123;&#125;        for go_id, n in cnt.items():            parents = self.get_parents(go_id)# 获取直接父功能            if len(parents) == 0:                min_n = n# 如果没有父功能，用自己的次数            else:                min_n = min([cnt[x] for x in parents])# 找父功能中最小的次数            self.ic[go_id] = math.log(min_n / n, 2)# 计算IC        def get_ic(self, go_id):        if self.ic is None:            raise Exception(&#x27;Not yet calculated&#x27;)        if go_id not in self.ic:            return 0.0        return self.ic[go_id]    #把文本格式的OBO文件转换成内存中的结构化数据    def load(self, filename, with_rels):        #ont 是一个字典        #ont[obj[&#x27;id&#x27;]] 是字典的键值对，键：obj[&#x27;id&#x27;]，值：obj这个对象        ont = dict()        obj = None        with open(filename, &#x27;r&#x27;) as f:            for line in f:                line = line.strip()                if not line:                    continue                if line == &#x27;[Term]&#x27;:# 开始一个新的术语块                    if obj is not None:                        ont[obj[&#x27;id&#x27;]] = obj# 如果已经有在处理的术语，先保存                    obj = dict()# 创建新的术语对象并初始化                    obj[&#x27;is_a&#x27;] = list()                    obj[&#x27;part_of&#x27;] = list()                    obj[&#x27;regulates&#x27;] = list()                    obj[&#x27;alt_ids&#x27;] = list()                    obj[&#x27;is_obsolete&#x27;] = False                    continue                elif line == &#x27;[Typedef]&#x27;:                    obj = None                else:                    if obj is None:                        continue                    l = line.split(&quot;: &quot;)# 分割键值对                    if l[0] == &#x27;id&#x27;:                        obj[&#x27;id&#x27;] = l[1]                    elif l[0] == &#x27;alt_id&#x27;:                        obj[&#x27;alt_ids&#x27;].append(l[1])                    elif l[0] == &#x27;namespace&#x27;:                        obj[&#x27;namespace&#x27;] = l[1]                    elif l[0] == &#x27;is_a&#x27;:                        obj[&#x27;is_a&#x27;].append(l[1].split(&#x27; ! &#x27;)[0])                    elif with_rels and l[0] == &#x27;relationship&#x27;:                        it = l[1].split()                        # add all types of relationships                        if it[0] == &#x27;part_of&#x27;:                            obj[&#x27;is_a&#x27;].append(it[1])                                                elif l[0] == &#x27;name&#x27;:                        obj[&#x27;name&#x27;] = l[1]                    elif l[0] == &#x27;is_obsolete&#x27; and l[1] == &#x27;true&#x27;:                        obj[&#x27;is_obsolete&#x27;] = True        if obj is not None:            ont[obj[&#x27;id&#x27;]] = obj        #处理别名和过时术语        for term_id in list(ont.keys()):# 遍历本体中的所有术语            for t_id in ont[term_id][&#x27;alt_ids&#x27;]:# 遍历每个术语的所有别名                ont[t_id] = ont[term_id]# 把别名的术语指向本体中的原始术语            if ont[term_id][&#x27;is_obsolete&#x27;]:# 如果术语是过时术语就删掉                del ont[term_id]        #构造父子关系        for term_id, val in ont.items():            if &#x27;children&#x27; not in val:                val[&#x27;children&#x27;] = set()            for p_id in val[&#x27;is_a&#x27;]:# 遍历每个术语的所有父节点                if p_id in ont:# 如果父节点存在于本体中            # 把当前术语添加到父节点的children集合中                    if &#x27;children&#x27; not in ont[p_id]:                        ont[p_id][&#x27;children&#x27;] = set()                    ont[p_id][&#x27;children&#x27;].add(term_id)        return ont    def get_anchestors(self, term_id):        if term_id not in self.ont:            return set()        term_set = set()        q = deque()        q.append(term_id)        while(len(q) &gt; 0):            t_id = q.popleft()            if t_id not in term_set:                term_set.add(t_id)                for parent_id in self.ont[t_id][&#x27;is_a&#x27;]:                    if parent_id in self.ont:                        q.append(parent_id)        return term_set    #只返回直接父节点    def get_parents(self, term_id):        if term_id not in self.ont:            return set()        term_set = set()        for parent_id in self.ont[term_id][&#x27;is_a&#x27;]:            if parent_id in self.ont:                term_set.add(parent_id)        return term_set    #获取指定命名空间下的所有术语    def get_namespace_terms(self, namespace):        terms = set()        for go_id, obj in self.ont.items():            if obj[&#x27;namespace&#x27;] == namespace:                terms.add(go_id)        return terms    #获取指定术语的命名空间    def get_namespace(self, term_id):        if term_id in self.ont:            return self.ont[term_id][&#x27;namespace&#x27;]        else:            return &#x27;can not find&#x27;        #使用BFS获取术语的所有后代节点（包括自身）    def get_term_set(self, term_id):        if term_id not in self.ont:            return set()        term_set = set()        q = deque()        q.append(term_id)        while len(q) &gt; 0:            t_id = q.popleft()            if t_id not in term_set:                term_set.add(t_id)                for ch_id in self.ont[t_id][&#x27;children&#x27;]:                    q.append(ch_id)        return term_set#为训练集生成三种类型的文件，用于后续的模型训练with open(&#x27;/home/wbshi/work/yunyanshuai/benchmark/DataProcess/interpro_feature_PPI/PPI_dict_index_protein.pkl&#x27;,&#x27;rb&#x27;) as fr:    dict_ppi_index_protein=pkl.load(fr)    #将索引映射到蛋白质ID的字典    #&#123;0: &#x27;P12345&#x27;, 1: &#x27;Q67890&#x27;, 2: &#x27;A1B2C3&#x27;, ...&#125;with open(&#x27;./data/ppi_pid_list.txt&#x27;,&#x27;w&#x27;) as fw:    for i in range(len(dict_ppi_index_protein)):        fw.write(dict_ppi_index_protein[i]+&quot;\\n&quot;)    #创建一个包含所有蛋白质ID的txt文件    f=open(&#x27;../../DataProcess/inputdata/train_data_separate.pkl&#x27;,&#x27;rb&#x27;)train=pkl.load(f)#读取训练集go_file=&#x27;../../DataProcess/inputdata/go.obo&#x27;go = Ontology(go_file, with_rels=True)#创建一个Ontology对象#生成18个文件for class_type in [&#x27;test1&#x27;,&#x27;test2&#x27;]:    for class_tag in [&#x27;bp&#x27;,&#x27;cc&#x27;,&#x27;mf&#x27;]:        #提取名为 proteins 的这一列test_proteins_list=list(pd.read_csv(&#x27;../../DataProcess/inputdata/&#123;0&#125;_data_separate_&#123;1&#125;_proteins.csv&#x27;.format(class_type,class_tag))[&#x27;proteins&#x27;])         with open(&#x27;../../DataProcess/inputdata/&#123;0&#125;_data_separate.pkl&#x27;.format(class_type),&#x27;rb&#x27;) as fr:            test_one=pkl.load(fr)                    with open(&#x27;./data/&#123;0&#125;_&#123;1&#125;_pid_list.txt&#x27;.format(class_tag,class_type), &#x27;w&#x27;) as f:             for protein in test_proteins_list:                f.write(f&#x27;&#123;protein&#125;\\n&#x27;)                with open(&#x27;./data/&#123;0&#125;_&#123;1&#125;_go.txt&#x27;.format(class_tag,class_type), &#x27;w&#x27;) as f:                for protein in test_proteins_list:                value = test_one[protein]  # 获取蛋白质数据                for go_id in value[f&#x27;all_&#123;class_tag&#125;&#x27;]:  # 遍历该蛋白质的所有功能                    annot_set = go.get_anchestors(go_id)  # 获取功能的所有上级                    for annot_id in annot_set:  # 遍历所有上级功能                        if go.get_namespace(annot_id) == NAMESPACES[class_tag]:  # 检查类别                            f.write(f&#x27;&#123;protein&#125;\\t&#123;annot_id&#125;\\t&#123;class_tag&#125;\\n&#x27;)  # 写入关系                                    with open(&#x27;./data/&#123;0&#125;_&#123;1&#125;.fasta&#x27;.format(class_tag,class_type), &#x27;w&#x27;) as f:            for protein in test_proteins_list:                value=test_one[protein]                seq=value[&#x27;sequences&#x27;]                f.write(f&#x27;&gt;&#123;protein&#125;\\n&#123;seq&#125;\\n&#x27;)                for class_type in [&#x27;train&#x27;]:    for class_tag in [&#x27;bp&#x27;,&#x27;cc&#x27;,&#x27;mf&#x27;]:        train_all_proteins_list=list(pd.read_csv(&#x27;../../DataProcess/inputdata/&#123;0&#125;_data_separate_&#123;1&#125;_proteins.csv&#x27;.format(class_type,class_tag))[&#x27;proteins&#x27;])        with open(&#x27;./data/&#123;0&#125;_&#123;1&#125;_pid_list.txt&#x27;.format(class_tag,class_type), &#x27;w&#x27;) as f:            for protein in train_all_proteins_list:                f.write(f&#x27;&#123;protein&#125;\\n&#x27;)        with open(&#x27;./data/&#123;0&#125;_&#123;1&#125;_go.txt&#x27;.format(class_tag,class_type), &#x27;w&#x27;) as f:              for protein in train_all_proteins_list:                value=train[protein]                for go_id in value[f&#x27;all_&#123;class_tag&#125;&#x27;]:                    annot_set = go.get_anchestors(go_id)                    for annot_id in annot_set:                        if go.get_namespace(annot_id) == NAMESPACES[class_tag]:                            f.write(f&#x27;&#123;protein&#125;\\t&#123;annot_id&#125;\\t&#123;class_tag&#125;\\n&#x27;)        with open(&#x27;./data/&#123;0&#125;_&#123;1&#125;.fasta&#x27;.format(class_tag,class_type), &#x27;w&#x27;) as f:            for protein in train_all_proteins_list:                value=train[protein]                seq=value[&#x27;sequences&#x27;]                f.write(f&#x27;&gt;&#123;protein&#125;\\n&#123;seq&#125;\\n&#x27;)\n\n","slug":"DFMB-generate_data","date":"2025-10-29T16:00:00.000Z","categories_index":"DeepFMB","tags_index":"项目","author_index":"Qushubiao"},{"id":"024d1f7df937a3339d4af68acb9edd21","title":"DFMB-generate_esm_graph","content":"\n这个主要是用来生成esm特征的。将每个蛋白质当成索引，找到他们的esm特征向量，把行号用蛋白质的名称来表示，列的数量不一定，因为每个特征维度的大小不同，将其转化成csr稀疏矩阵，然后保存为npz文件。\n","slug":"DFMB-generate_esm_graph","date":"2025-10-27T16:00:00.000Z","categories_index":"DeepFMB","tags_index":"项目","author_index":"Qushubiao"},{"id":"f754d82233e599ebfb0e644c4bd14184","title":"DFMB-generateGraph","content":"\n首先通过dict_ppi_protein_index来建立蛋白质ID与数值索引的对应关系，下一步要将final_all_filter_300_graph_combine_score.txt里面的蛋白质作用关系转化为稀疏行格式的邻接矩阵，其中要防止边的重复，格式为  数值&#x2F;行&#x2F;列。\n将这个CSR图保存为npz格式的文件。\n\n这个要用有向图生成无向图，所以要保证两个端点间有两条边。\n","slug":"DFMB-generateGraph","date":"2025-10-26T16:00:00.000Z","categories_index":"DeepFMB","tags_index":"项目","author_index":"Qushubiao"},{"id":"f8dca93546ae5a2408fda06940c718d8","title":"ProtGO","content":"ProtGO1、模型架构ProtGO 是一个三阶段多模态融合框架，旨在整合 Gene Ontology 知识库中的四种模态信息，提升蛋白质功能预测性能，其核心模块整合了以下四个部分：\n\n蛋白质序列表示：使用预训练蛋白质语言模型（PLMs）如 ESM-2、Ankh、xTrimoPGLM 提取序列嵌入。\n文本对齐模块：利用生物医学语言模型（如 BioMedBERT）提取文本描述嵌入，通过余弦相似度筛选相关文本，并使用多头自注意力机制与序列表示融合。\n分类编码模块：使用标签编码将物种分类信息嵌入到特征中，增强物种特异性表示。\nGO关系图模块：基于图神经网络（GNN）对 GO 术语之间的有向无环图结构进行建模，提升多标签分类性能。\n\n其预测方式支持两种策略：线性探测 + MLP（预训练PLMs的参数被完全锁定，其作为通用蛋白质序列编码器的功能保持不变。大规模预训练PLMs已蕴含了足够丰富的序列语义与进化信息，蛋白质功能预测任务的关键在于如何构建一个有效的“翻译层”与“融合器”，以解码并充分利用PLMs的固有知识，而非改变其知识本身。）微调 + LoRA（该策略不直接对PLMs庞大的原始参数进行全量更新，而是在其Transformer架构中注入少量的、可训练的低秩矩阵，通过调整前向传播过程来实现对PLMs行为的轻微引导。）\n然而，在ProtGO的具体实践中，LoRA带来的额外性能增益（平均≤1.5%）远低于线性探测，同时其训练成本仍高于后者。这一现象表明，对于蛋白质功能预测这一任务，通过外部多模态架构进行“信息增强”的价值，已超过了通过对PLMs本身进行“参数微调”所能带来的边际收益。\n2、数据集主要数据集CAFA5（第5届蛋白质功能注释评估）：样本数：142,246 条蛋白质序列，GO 术语数：31,466 ，物种数：3,156 种，划分：128,020 训练样本 &#x2F; 14,225 测试样本\n对数据预处理并非数据集中每一个蛋白质样本都完整拥有全部四种模态的信息。如果一个蛋白质缺少某种模态（例如没有文本描述），那么对应的模块（文本对齐模块）就可以被跳过，框架依然可以基于可用的模态进行计算。去除低频 GO 术语（频率 &lt; 0.015%），缓解长尾效应，显著提升了模型在主流功能上的整体性能和稳定性，并大幅降低了计算复杂度和内存开销。 通过五折交叉验证确保了实验结果的稳健性。这些对数据细节的周密处理，为其最终获得的卓越性能提供了坚实保障，也为其框架的通用性和可靠性提供了有力支持。\n3、实验结果数据在指标Fmax上，ProtGO与未经增强的基础蛋白质语言模型相比，ProtGO 增强后的模型在 Fmax 指标上实现了约 8% 至 27% 的显著增长。这一提升在不同架构和规模的PLM上均得以体现，具有普适性。尤为突出的是，一个相对轻量的模型在 ProtGO 的赋能下，其性能可以超越未被增强的、规模大得多的模型。例如，基于 ESM-150M 的 ProtGO 模型，其表现甚至优于参数规模达150亿的原始 ESM-15B 模型。\n与现有的专门化蛋白质功能预测模型进行横向比较时，ProtGO 的优势同样明显。在相同的实验设置下，ProtGO 显著超越了包括 OntoProtein、DeepFRI、ProtST、DeepGOGAT 在内的多个先进基准模型。与现有最佳方法相比，ProtGO 通过全面融合四种模态信息，性能显著优于仅整合部分模态的上述模型。\n消融实验结果显示，所有新增模态均对性能提升有正向作用，但其贡献度存在差异。其中，其中 GO 关系图作用最为关键，文本描述次之，物种信息辅助作用相对有限。此外，研究还发现任意两种模态的组合其效果均优于单一模态。\nProtGO 默认采用的线性探测策略，其表现与更为计算密集的微调策略不相上下，甚至在部分指标上更为优越。它表明性能提升的主要来源并非通过对基础PLM进行任务特定的参数调整，而是源于 ProtGO 框架本身强大的多模态信息整合能力。该框架能够高效地挖掘和利用PLM中已存在的通用知识，从而实现了更优的性能与计算效率的平衡。\n生物学验证进一步支持了其预测可靠性：ProtGO 能准确定位蛋白结构功能位点，其预测的功能相似蛋白对在相互作用数据库中验证匹配率高达 69.7%，证实了预测结果的生物学相关性。\n4、贡献与创新点提出首个全面整合四模态GO知识的通用框架，系统融合蛋白序列、文本描述、物种分类与GO关系图，突破以往方法仅使用部分模态的局限性。\n设计模块化、轻量级的自适应架构，各模态处理模块独立灵活，可适配不同规模的PLM与生物LM，并能有效处理现实场景中模态缺失问题。\n验证了多模态融合相较于模型微调的有效性，在线性探测下性能显著提升，表明其能力主要源于信息整合架构而非参数优化。\n","slug":"ProtGO","date":"2025-10-23T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"9e921fadc0b1d8f1e892add2c28e6609","title":"PO2GO","content":"PO2GO1、模型架构在模型架构上，PO2Vec首次利用GO有向无环图中基于最短可达路径的偏序关系，定义直接可达（如父子关系）、间接可达（如兄弟关系）和不可达（不同领域）三种语义关系，通过分层负采样和改进的平衡InfoNCE损失进行对比学习，使术语嵌入能精确反映其在GO层次中的位置与亲疏关系；蛋白质特征则由ESM-1b预训练语言模型提取，经均值池化得到全局嵌入；最终通过联合建模预测将蛋白质与GO术语嵌入投影至同一语义空间，使用多标签交叉熵损失进行端到端训练，实现功能注释预测。\n2、数据集GO结构数据\n使用GO的有向无环图结构，仅考虑 is_a 和 part_of 两种关系（占88%）。\n涵盖三个子本体：BPO、MFO、CCO。\n\n蛋白质功能预测评估数据\nCAFA3：国际蛋白质功能预测评估数据集。\nSwissProt：高质量标注的蛋白质数据库。\nPFAM 和 PPI 数据集：用于评估GO嵌入与蛋白质功能域、相互作用的关联性。用来额外检验PO2Vec学到的是否真的能反映出蛋白质的功能相似性和相互作用关系。\n\n\n3、实验结果数据、实验结果显示，PO2Vec在GO术语深度预测任务中准确率显著优于基线模型（Anc2Vec、Opa2Vec、TransH等），1-Wasserstein距离达0.41，能清晰分离不同本体领域的术语；在蛋白质功能预测中，PO2GO在CAFA3基准上取得BPO、MFO、CCO的Fmax分别为0.526、0.611、0.648，全面领先现有方法；其预测结果具有更高信息量，在BPO和MFO上的平均信息内容分别为9.46和5.78，且在训练样本稀缺的场景下性能下降最小，展现出强大的泛化与实用价值。\n消融实验移除PO2Vec或替换为多热编码后，性能显著下降。固定PO2Vec嵌入不微调时，性能仍保持良好，说明预训练嵌入质量高。\n\n4、贡献与创新点PO2Vec：首个利用最短可达路径偏序关系进行GO术语嵌入学习的方法。\nBalanced InfoNCE：提出分层采样策略，解决负样本不平衡问题。\nPO2GO：将PO2Vec与ESM-1b结合，实现端到端的蛋白质功能预测。\nPO2Vec方法通过引入最短可达路径的偏序约束和对比学习策略，显著提升了GO术语嵌入的质量。基于PO2Vec构建的PO2GO模型在多个基准数据集上表现出优越的蛋白质功能预测能力，尤其是在预测特异性、少样本学习和层次结构建模方面具有明显优势。该工作为生物医学本体的表示学习与蛋白质功能注释提供了新的思路和技术路径。\n","slug":"PO2GO","date":"2025-10-21T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"18dd88815610e232a76dd8748c77c34b","title":"GOBoost","content":"GOBoost1、模型架构GOBoost 是一个针对蛋白质功能预测中长尾分布问题设计的深度集成学习模型。使用三个基础模型：GOBoost Head：专注于高频 GO 术语；GOBoost Tail：专注于中低频 GO 术语;GOBoost All ：使用全部标签训练。对重叠的预测结果进行加权平均，提升对长尾标签的预测能力。\n模型输入使用 ESM-1b 提取蛋白质序列；使用 AlphaFold 2 预测蛋白质三维结构，构建残基接触图。\n图神经网络使用 GCN 对蛋白质图结构进行特征提取；使用 Class Activation Mapping 将结构特征映射为 GO 术语嵌入。\n全局-局部标签图模块分为两个部分：全局标签图：捕获高频 GO 术语之间的共现关系；局部标签图：针对每个蛋白质动态构建，补充低频 GO 术语的关系信息。\n损失函数使用多粒度焦点损失函数，对低频 GO 术语赋予更高权重，缓解类别不平衡问题。\n2、数据集PDB 数据集\n来源：DeepFRI 使用的实验结构数据。按照训练:验证:测试 &#x3D; 8:1:1进行划分。注释来源：SIFTS + UniProtKB。\nAF2 数据集由AlphaFold 2 预测得来的结构。其中序列相似性 ≤ 25%。每个蛋白质至少有一个 IC &gt; 10 的 GO 术语。\n3、实验结果数据PDB 数据集上的表现\n\n\n方法\nMF (AUPR)\nBP (AUPR)\nCC (AUPR)\nFmax (MF&#x2F;BP&#x2F;CC)\n\n\n\nHEAL\n0.691\n0.337\n0.467\n0.747&#x2F;0.595&#x2F;0.687\n\n\nGOBoost\n0.765\n0.458\n0.573\n0.787&#x2F;0.659&#x2F;0.745\n\n\n提升幅度\n+10.71%\n+35.91%\n+22.71%\n+5.35%&#x2F;10.76%&#x2F;8.44%\n\n\nAF2 数据集上的表现\n\n\n方法\nMF (AUPR)\nBP (AUPR)\nCC (AUPR)\n\n\n\nHEAL\n0.502\n0.200\n0.287\n\n\nGOBoost\n0.582\n0.246\n0.318\n\n\n提升幅度\n+15.64%\n+23.00%\n+10.80%\n\n\n不同 IC 值下的性能对 IC &gt; 10（最难预测的特定功能），GOBoost 在 BP 上的 AUPR 提升 **52.48%**，显著优于基线。\n4、贡献与创新点提出长尾优化集成策略，首次在蛋白质功能预测中集成多个深度学习模型以分别处理不同频率的标签。\n结合全局共现矩阵与局部动态图，有效捕获低频 GO 术语的关系。\n多粒度焦点损失函数：对低频标签赋予更高权重，提升模型对长尾功能的敏感度。\n","slug":"GOBoost","date":"2025-10-21T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"6337ea3a45c7ed79f51176d6bcc5ca76","title":"GOHPro","content":"GOHPro蛋白质功能相似性网络构建DomainNet（结构相似性网络）：基于蛋白质域组成（比较两个蛋白质自身拥有的结构域）和邻域上下文构建（比较两个蛋白质的邻居蛋白质所拥有的结构域），结合了上下文相似性和组成相似性，通过参数β平衡（β&#x3D;0.1）。\nModuleNet（模块相似性网络）：基于蛋白质复合物信息（Complex Portal）构建，使用超几何分布评估复合物的功能显著性。\nProteinNet（融合网络）：将 DomainNet 和 ModuleNet 线性融合，权重由参数γ控制（γ依 ontology 不同而调整）。\nGO语义相似性网络构建基于 GO 的层次结构（DAG），仅保留 is_a 和 part_of 关系其他关系可能更复杂或更模糊，保留它们会引入噪声。通过不同的权重计算 GO 术语间的相似性。\n异质网络整合将 ProteinNet、GO-GO 网络、蛋白质-GO 注释网络整合为一个异质网络 GPG。异质网络的价值在于让不同类型的节点通过不同类型的关系相互影响**。一个GO术语的信息可以通过语义关系影响另一个GO术语，同时也可以通过已知注释影响蛋白质，蛋白质再通过功能相似影响其他蛋白质这种跨类型的信息流动让预测更加智能。\n此时，功能未知的蛋白质也在异质网络中，它通过“蛋白-蛋白”边与其他蛋白紧密相连，和GO术语层是断开的。\n\n网络传播算法使用迭代传播公式：WPGt+1&#x3D;α⋅(WP⋅WPGt⋅WG)+(1−α)⋅WPG0，其中 α&#x3D;0.1α&#x3D;0.1，控制信息传播的衰减。所有已知的蛋白质-GO注释（档案）作为信息的起点，信息在 蛋白关系网 中传播：一个已知功能的蛋白，会把它的功能信息传递给与它相似的其他蛋白。信息同时会在 GO术语关系网 中传播：一个GO术语（如“酶活性”）的信息，会传递给与它相似的其他GO术语（如“催化活性”）。信息通过 已知注释 在蛋白层和GO层之间来回流动、相互增强。经过多次迭代，网络中的信息分布不再剧烈变化，达到稳定。最终根据传播后的权重排序，为未知蛋白质推荐 GO 注释。\n\n2、数据集训练和开发数据集：\n使用来自多个权威生物数据库的公开数据构建异质网络。蛋白质-蛋白质相互作用（PPI）数据来自 BioGRID（2024年10月版），包含酵母（3,162个蛋白）和人类（7,317个蛋白）的全物种互作网络。基因本体（GO）注释来自 GO Consortium（2024年9月版），并严格筛选仅保留具有实验证据代码的高质量注释，覆盖 Biological Process (BP), Molecular Function (MF), 和 Cellular Component (CC) 三个本体。蛋白质域信息来自 Pfam 37.0，蛋白质复合物信息来自 Complex Portal（2024年11月版）。在模型评估中，主要采用留一法交叉验证，即将数据集中每个蛋白质依次视为未知功能进行预测，其余蛋白质用于训练，以全面评估模型在已知数据分布下的性能。\n\n酵母：作为一个简单的真核模式生物，其蛋白质组注释最为完善，PPI网络也最密集。这为方法原理的验证提供了一个理想的“试验场”。\n人类：作为最终的应用目标，其蛋白质组规模更大、更复杂，PPI网络也更稀疏。这用于评估方法在真实、复杂场景下的性能和泛化能力。\n\n基准测试独立数据集：\nCAFA3基准测试集： 使用国际蛋白质功能注释挑战赛 CAFA3 的官方数据集进行前瞻性评估。使用CAFA3特定时间戳前的蛋白质和注释作为训练集，并将其后新注释的蛋白质作为独立的测试集。例如，对于人类物种，BP、MF、CC本体的训练集规模分别增至5,153、3,109、2,858个蛋白，而测试集规模分别为188、554、642个蛋白。该测试集模拟了预测全新发现蛋白质功能的真实场景，有效评估模型的泛化能力。\n案例验证数据集：\n共享结构域蛋白集： 精选4个共享AAA+ATPase结构域但功能各异的蛋白质（如人源PEX6、VCP；酵母ORC1、YTA12），用于验证模型在区分功能细微差别和解决功能歧义方面的能力。\n网络扰动测试集： 通过对上述案例蛋白系统地删除其关键结构域或相互作用，构建扰动数据集，用于分析模型预测结果对关键生物学特征的依赖性和鲁棒性。\n\n3、实验结果数据整体性能对比：\n在酵母和人类数据集上，与六种先进工具（exp2GO、GrAPFI-GO、PHN、DCS、NC、DeepGO-SE）进行比较，GOHPro在主要评估指标Fmax上展现出全面优势。\n酵母数据集： GOHPro 在 Biological Process (BP), Molecular Function (MF), 和 Cellular Component (CC) 上的 Fmax 分别达到 0.45, 0.548, 0.58。\n人类数据集： GOHPro 在 BP&#x2F;MF&#x2F;CC 上的 Fmax 分别为 0.261, 0.447, 0.354，其中在 Molecular Function (MF) 上的性能提升最为显著，相较次优方法提升幅度达 **47.5%**。\nCAFA3独立基准测试： 在更具挑战性的CAFA3基准上，GOHPro的泛化能力得到验证，在人类物种上的 Fmax 提升超过 **62%**。\n功能特异性与鲁棒性分析：\n共享结构域蛋白功能区分： 在四个共享AAA+ATPase结构域但功能各异的蛋白质案例中，GOHPro能准确预测出其不同的生物学功能，成功解决了功能歧义问题。\n敏感性分析： 实验表明，当去除蛋白质的关键相互作用或复合物成员信息时，GOHPro对相应功能的预测精度显著下降。\n低同源性与稀疏网络下的性能：\n低同源性（Dark）蛋白质表现： 对于同源性低（&lt;40%）、进化信息稀缺的“暗蛋白质”，GOHPro通过整合模块相似性网络，仍能提供有效的功能推断。DomainNet在其中发挥了关键的补充作用。\n网络连通性敏感性： 网络稀疏性实验显示，低同源性蛋白质对网络连接的完整性更为敏感。当90%的网络边被移除时，其Fmax性能下降高达76%，显著高于高同源性蛋白质的33% 下降幅度，凸显了其依赖密集网络进行信息传递的特性。\n\n4、贡献与创新点提出异质网络融合策略，整合了：蛋白质功能相似性+GO 语义相似性+已知蛋白质-GO 注释\n引入网络传播算法，实现全局信息扩散，缓解数据稀疏性问题。\n揭示了同源性和网络连通性对预测性能的影响，为 dark protein 注释提供新思路。\n在酵母和人类数据集上系统评估，涵盖多个评估指标。\nGOHPro 是一种基于异质网络传播的蛋白质功能预测方法，通过融合多源数据（PPI、域、复合物、GO语义）构建统一网络，并利用传播算法实现功能注释的全局优化。在酵母和人类数据集上，GOHPro 在多个评估指标上显著优于现有方法，尤其在区分共享结构域蛋白质的功能方面表现出色。其模块化设计和可扩展性为未来整合更多数据类型（如结构信息）奠定了基础，是蛋白质功能预测领域一项重要进展。\n","slug":"GOHPro","date":"2025-10-20T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"a51592e2b5599f8db32fcf648d257b16","title":"ProstaNet","content":"ProstaNet一.研究背景\n天然的蛋白质有时不够稳定\n实验方法测定突变对稳定性的影响成本高、耗时长。\n现有计算方法分为基于物理模型和基于机器学习的方法，但存在数据偏差、结构表示不全面、对多点突变预测能力弱等问题。\n\n二.研究问题\n如何提高蛋白质稳定性预测的准确性，尤其是对于多点突变？\n如何解决当前数据量不足、数据偏差大、缺乏多点突变基准测试集的问题？\n如何有效结合几何信息与关系信息来提升模型性能？\n\n三.研究目的开发一个高精度的蛋白质热稳定性预测模型，能够同时处理单点突变和多点突变。\n四.研究方法\n数据构建：\n从公共数据集上收集原始数据，去噪后构建ProstaDB，包含3,784个单点突变和1,642个多点突变。\n提出热力学循环 数据增强方法，像高中学的热化学反应方程式，能从已有的数据里推理和创造出新的、合理的数据，扩充多点突变数据至7,360条。\n使用反称性增强 ，利用数据的对称性来平衡正负样本，让模型的预测没有偏见。\n\n\n模型设计：\n用GVP-GNN 提取结构图中的几何与关系特征，能够更全面、更精确地“感受”到蛋白质结构的微妙变化，从而在预测突变影响时，比只关注一种信息的模型准确得多。而且对于每个节点，会从邻居处获取信息，这个信息包含节点特征和边特征，在空间中还会保持等变性，即无论是旋转还是平移，这些特征都会跟着变化。\n节点特征（描述一个氨基酸“自身”）：\n标量特征 S_v：描述氨基酸的内在属性。比如：它是哪种氨基酸（用One-hot编码）、它的疏水性得分、它的进化保守性（PSSM）等。这些是没有方向的信息。\n向量特征 V_v：描述氨基酸在空间中的朝向和方向。具体来说，就是其主链上的化学键方向（如Cα-C键的方向、Cα-N键的方向）。这些是有方向的信息，就像一个小箭头。\n\n\n边特征（描述两个氨基酸之间的“关系”）：\n标量特征 S_e：描述关系的强度或距离。最直接的就是两个氨基酸Cα原子之间的欧氏距离。\n向量特征 V_e：描述两个氨基酸之间的相对位置和方向。通常就是从一个原子指向另一个原子的方向向量。\n\n\n结合多头注意力机制 比较野生型与突变型之间的差异。\n预测结果从具体的ΔΔG值，转化为稳定与否的二元问题。\n采用预训练（单点）→ 微调（多点） 的训练策略，用大数据（单点突变）预训练，用小数据（多点突变）微调。让模型学习当多个单点突变同时发生时，它们之间是如何相互影响的，是否会有1+1&gt;2？\n\n\n特征编码：\n评估7类氨基酸编码方法（如残基打分、进化特征、物理化学属性等），发现残基打分对预测性能影响最大。\n\n\n聚类方法：使用t-SNE + DBSCAN 对突变类型聚类，将不同的多点突变组合按类型分组，确保训练集与测试集的突变类型组合分布一致。（类似分层抽样）\n\n五.分析方法\n消融实验：评估不同编码方法对模型性能的贡献。\n对比实验：与ThermoMPNN、FoldX、PoPMuSiC等9种现有方法比较。\n实验验证：通过圆二色谱验证HuJ3纳米抗体突变的稳定性预测结果。\n\n六.创新点\n首次将GVP-GNN应用于蛋白质稳定性预测，结合几何与关系特征。\n构建高质量数据库ProstaDB，涵盖单点与多点突变，并提出TL数据增强方法。\n提出聚类方法构建多点突变测试集，解决基准缺失问题。\n系统评估氨基酸编码方法，明确残基打分和PSSM在多点突变中的关键作用。\n实验验证：通过HuJ3纳米抗体突变实验验证模型实用性。\n\n七.局限性\n数据限制：训练数据量仍不足，尤其是单点突变缺乏有效的数据增强方法。\n结构依赖：模型依赖晶体结构，而实际中多使用AlphaFold2预测结构，存在偏差。\n突变结构建模：Rosetta Fast Relax无法模拟大构象变化，影响突变结构准确性。\n\n","slug":"ProstaNet (1)","date":"2025-10-15T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"99cc996dc0617c9203fb12b8cb30ba1d","title":"Kaggle房价预测","content":"Kaggle房价预测12345678# 若无法获得测试数据，则可根据训练数据计算均值和标准差numeric_features = all_features.dtypes[all_features.dtypes != &#x27;object&#x27;].index# 对每个数字特征进行标准化，把数据变成以0为中心，标准差为1的分布# 数据围绕着0分布，可以将不同尺度的特征放在一起比较，防止差异性带来的权重不平衡问题all_features[numeric_features] = all_features[numeric_features].apply(    lambda x: (x - x.mean()) / (x.std()))# 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0all_features[numeric_features] = all_features[numeric_features].fillna(0)\n\n\n查看每个特征的数据类型，为后续数据预处理做准备\n\n123456# &quot;Dummy_na=True&quot;将&quot;na&quot;（缺失值）视为有效的特征值，并为其创建指示符特征all_features = pd.get_dummies(all_features, dummy_na=True)#将类别特征转换为数值形式（独热编码）#独热编码可以消除数学大小关系，遇到任何新的类别值，就给所有数据创建对应的列#但会导致特征爆炸性增长all_features = pd.get_dummies(all_features, dummy_na=True)\n\n\n1234567#就是一个线性模型，每个参数的权重不一样loss = nn.MSELoss()in_features = train_features.shape[1]def get_net():    net = nn.Sequential(nn.Linear(in_features,1))    return net\n\n1234567def log_rmse(net, features, labels):    # 为了在取对数时进一步稳定该值，将小于1的值设置为1    clipped_preds = torch.clamp(net(features), 1, float(&#x27;inf&#x27;))    rmse = torch.sqrt(loss(torch.log(clipped_preds),                           torch.log(labels)))    return rmse.item()# 不看绝对数量，看相对比例\n\n1234567891011121314151617181920def train(net, train_features, train_labels, test_features, test_labels,          num_epochs, learning_rate, weight_decay, batch_size):    train_ls, test_ls = [], []    train_iter = d2l.load_array((train_features, train_labels), batch_size)    # 这里使用的是Adam优化算法,还可以防止过拟合    optimizer = torch.optim.Adam(net.parameters(),                                 lr = learning_rate,                                 weight_decay = weight_decay)    for epoch in range(num_epochs):        for X, y in train_iter:            #训练的老套路            optimizer.zero_grad()            l = loss(net(X), y)            l.backward()            optimizer.step()        train_ls.append(log_rmse(net, train_features, train_labels))        if test_labels is not None:            test_ls.append(log_rmse(net, test_features, test_labels))    return train_ls, test_ls#返回训练结果\n\n12345678910111213141516171819202122232425262728293031323334def get_k_fold_data(k, i, X, y):    assert k &gt; 1    fold_size = X.shape[0] // k    X_train, y_train = None, None    for j in range(k):        idx = slice(j * fold_size, (j + 1) * fold_size)        X_part, y_part = X[idx, :], y[idx]        if j == i:            X_valid, y_valid = X_part, y_part        elif X_train is None:            X_train, y_train = X_part, y_part        else:            X_train = torch.cat([X_train, X_part], 0)            y_train = torch.cat([y_train, y_part], 0)    return X_train, y_train, X_valid, y_validdef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,           batch_size):    train_l_sum, valid_l_sum = 0, 0    for i in range(k):        data = get_k_fold_data(k, i, X_train, y_train)        net = get_net()        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,                                   weight_decay, batch_size)        train_l_sum += train_ls[-1]        valid_l_sum += valid_ls[-1]        if i == 0:            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],                     xlabel=&#x27;epoch&#x27;, ylabel=&#x27;rmse&#x27;, xlim=[1, num_epochs],                     legend=[&#x27;train&#x27;, &#x27;valid&#x27;], yscale=&#x27;log&#x27;)        print(f&#x27;折&#123;i + 1&#125;，训练log rmse&#123;float(train_ls[-1]):f&#125;, &#x27;              f&#x27;验证log rmse&#123;float(valid_ls[-1]):f&#125;&#x27;)    return train_l_sum / k, valid_l_sum / k#K折训练算法：将训练集人为的划分成几份，利用其中一份进行验证，剩余的拿来训练。依次充当验证集，最后取平均值，既充分利用了数据，又得到了最公平的模型评估。\n\n12345k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,                          weight_decay, batch_size)print(f&#x27;&#123;k&#125;-折验证: 平均训练log rmse: &#123;float(train_l):f&#125;, &#x27;      f&#x27;平均验证log rmse: &#123;float(valid_l):f&#125;&#x27;)\n\n\n","slug":"Kaggle房价预测","date":"2025-10-13T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"},{"id":"0ef6094797649f91ccbc2e9b364eb919","title":"序列数据","content":"序列数据12345678T = 1000  # 总共产生1000个点time = d2l.arange(1, T + 1, dtype=d2l.float32)#定义时间序列的长度（横轴）为1000个点x = d2l.sin(0.01 * time) + d2l.normal(0, 0.2, (T,))#生成正弦波并且添加高斯噪声d2l.plot(time, [x], &#x27;time&#x27;, &#x27;x&#x27;, xlim=[1, 1000], figsize=(6, 3))#x轴数据、y轴数据、x轴标签、y轴标签、x轴显示范围、图表尺寸\n\n\n1234epoch：训练的完整周期batch/mini-batch：分批训练Iterations（迭代次数）也就是batch的数量batchSize：每个batch包含的样本数量\n\n1234567891011121314tau = 4features = d2l.zeros((T - tau, tau))#特征为996行，4列for i in range(tau):    features[:, i] = x[i: T - tau + i]#初始化特征矩阵labels = d2l.reshape(x[tau:], (-1, 1))#重塑一下标签矩阵#@tab allbatch_size, n_train = 16, 600# 只有前n_train个样本用于训练train_iter = d2l.load_array((features[:n_train], labels[:n_train]),                            batch_size, is_train=True)#创建好数据加载器，用于模型训练，用前600个样本，每次训练使用16个样本\n\n12345678910111213141516# 初始化网络权重的函数#如果是线性层，就要用Xavier方法初始化权重，自动计算合适的初始化范围def init_weights(m):    if type(m) == nn.Linear:        nn.init.xavier_uniform_(m.weight)# 一个简单的多层感知机def get_net():    net = nn.Sequential(nn.Linear(4, 10),#输入4个特征，输出10个神经元                        nn.ReLU(),#激活函数，非线性                        nn.Linear(10, 1))#通过10个神经元，输出一个预测值    net.apply(init_weights)#给所有层应用权重初始化    return net# 平方损失。注意：MSELoss计算平方误差时不带系数1/2loss = nn.MSELoss(reduction=&#x27;none&#x27;)\n\n123456789101112131415def train(net, train_iter, loss, epochs, lr):    trainer = torch.optim.Adam(net.parameters(), lr)    #这是一个高度封装的的训练器，只需要传进去网络参数以及学习率就可以    for epoch in range(epochs):        for X, y in train_iter:            #训练的套路基本都是固定的            trainer.zero_grad()            l = loss(net(X), y)            l.sum().backward()            trainer.step()        print(f&#x27;epoch &#123;epoch + 1&#125;, &#x27;              f&#x27;loss: &#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;&#x27;)net = get_net()train(net, train_iter, loss, 5, 0.01)\n\n\n1234567onestep_preds = net(features)#调用的是net.__call__(features)#得到996个预测值d2l.plot([time, time[tau:]],         [d2l.numpy(x), d2l.numpy(onestep_preds)], &#x27;time&#x27;,         &#x27;x&#x27;, legend=[&#x27;data&#x27;, &#x27;1-step preds&#x27;], xlim=[1, 1000],         figsize=(6, 3))\n\n\n123456789101112131415multistep_preds = d2l.zeros(T)multistep_preds[: n_train + tau] = x[: n_train + tau]#前面的604个值都用真实数据填充for i in range(n_train + tau, T):    multistep_preds[i] = net(        d2l.reshape(multistep_preds[i - tau: i], (1, -1)))    #从第605开始，到1000，就开始用前四个来计算当前的值    #误差会累积，所以越来越偏离#@tab alld2l.plot([time, time[tau:], time[n_train + tau:]],         #原始数据、从第五个点开始：单步预测、从第605个点开始：多步预测         [d2l.numpy(x), d2l.numpy(onestep_preds),          d2l.numpy(multistep_preds[n_train + tau:])], &#x27;time&#x27;,         &#x27;x&#x27;, legend=[&#x27;data&#x27;, &#x27;1-step preds&#x27;, &#x27;multistep preds&#x27;],         xlim=[1, 1000], figsize=(6, 3))\n\n\n123456789101112131415161718#@tab allmax_steps = 64#@tab mxnet, pytorchfeatures = d2l.zeros((T - tau - max_steps + 1, tau + max_steps))# 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）for i in range(tau):    features[:, i] = x[i: i + T - tau - max_steps + 1]# 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）for i in range(tau, tau + max_steps):    features[:, i] = d2l.reshape(net(features[:, i - tau: i]), -1)#%%#只选择了4个有代表性的步长steps = (1, 4, 16, 64)d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],         [d2l.numpy(features[:, tau + i - 1]) for i in steps], &#x27;time&#x27;, &#x27;x&#x27;,         legend=[f&#x27;&#123;i&#125;-step preds&#x27; for i in steps], xlim=[5, 1000],         figsize=(6, 3))\n\n\n","slug":"序列数据","date":"2025-10-11T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"},{"id":"dd8e371b161018fe37f17369405b9122","title":"深度学习笔记","content":"深度学习\n梯度下降算法：用来优化参数，让模型的预测更准确\n\n1.1  计算梯度\n1.2  更新参数（需要设置好学习率，就是步长）\n1.3  不断重复，直到梯度为零\n新的参数 &#x3D; 旧的参数 - 学习率 × (损失函数在旧参数点的梯度)\n\n最终目标是要使损失函数最小\n训练过程：\n\n3.1 前向传播\n3.2 计算损失（交叉熵损失）\n3.3 反向传播\n3.4 参数更新\n3.5 循环迭代\n\n激活函数：引入非线性（不同任务类型要用不同的激活函数）\nCNN\n\n输入-卷积-激活函数（简单特征到复杂特征的非线性映射）-池化（简化特征）-展平-全连接-输出\n","slug":"深度学习笔记","date":"2025-09-27T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"}]