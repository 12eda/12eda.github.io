[{"id":"18dd88815610e232a76dd8748c77c34b","title":"GOBoost","content":"GOBoost1、模型架构GOBoost 是一个针对蛋白质功能预测中长尾分布问题设计的深度集成学习模型。使用三个基础模型：GOBoost Head：专注于高频 GO 术语；GOBoost Tail：专注于中低频 GO 术语;GOBoost All ：使用全部标签训练。对重叠的预测结果进行加权平均，提升对长尾标签的预测能力。\n模型输入使用 ESM-1b 提取蛋白质序列；使用 AlphaFold 2 预测蛋白质三维结构，构建残基接触图。\n图神经网络使用 GCN 对蛋白质图结构进行特征提取；使用 Class Activation Mapping 将结构特征映射为 GO 术语嵌入。\n全局-局部标签图模块分为两个部分：全局标签图：捕获高频 GO 术语之间的共现关系；局部标签图：针对每个蛋白质动态构建，补充低频 GO 术语的关系信息。\n损失函数使用多粒度焦点损失函数，对低频 GO 术语赋予更高权重，缓解类别不平衡问题。\n2、数据集PDB 数据集\n来源：DeepFRI 使用的实验结构数据。按照训练:验证:测试 &#x3D; 8:1:1进行划分。注释来源：SIFTS + UniProtKB。\nAF2 数据集由AlphaFold 2 预测得来的结构。其中序列相似性 ≤ 25%。每个蛋白质至少有一个 IC &gt; 10 的 GO 术语。\n3、实验结果数据PDB 数据集上的表现\n\n\n方法\nMF (AUPR)\nBP (AUPR)\nCC (AUPR)\nFmax (MF&#x2F;BP&#x2F;CC)\n\n\n\nHEAL\n0.691\n0.337\n0.467\n0.747&#x2F;0.595&#x2F;0.687\n\n\nGOBoost\n0.765\n0.458\n0.573\n0.787&#x2F;0.659&#x2F;0.745\n\n\n提升幅度\n+10.71%\n+35.91%\n+22.71%\n+5.35%&#x2F;10.76%&#x2F;8.44%\n\n\nAF2 数据集上的表现\n\n\n方法\nMF (AUPR)\nBP (AUPR)\nCC (AUPR)\n\n\n\nHEAL\n0.502\n0.200\n0.287\n\n\nGOBoost\n0.582\n0.246\n0.318\n\n\n提升幅度\n+15.64%\n+23.00%\n+10.80%\n\n\n不同 IC 值下的性能对 IC &gt; 10（最难预测的特定功能），GOBoost 在 BP 上的 AUPR 提升 **52.48%**，显著优于基线。\n4、贡献与创新点提出长尾优化集成策略，首次在蛋白质功能预测中集成多个深度学习模型以分别处理不同频率的标签。\n结合全局共现矩阵与局部动态图，有效捕获低频 GO 术语的关系。\n多粒度焦点损失函数：对低频标签赋予更高权重，提升模型对长尾功能的敏感度。\n","slug":"GOBoost","date":"2025-10-21T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"9e921fadc0b1d8f1e892add2c28e6609","title":"PO2GO","content":"PO2GO1、模型架构在模型架构上，PO2Vec首次利用GO有向无环图中基于最短可达路径的偏序关系，定义直接可达（如父子关系）、间接可达（如兄弟关系）和不可达（不同领域）三种语义关系，通过分层负采样和改进的平衡InfoNCE损失进行对比学习，使术语嵌入能精确反映其在GO层次中的位置与亲疏关系；蛋白质特征则由ESM-1b预训练语言模型提取，经均值池化得到全局嵌入；最终通过联合建模预测将蛋白质与GO术语嵌入投影至同一语义空间，使用多标签交叉熵损失进行端到端训练，实现功能注释预测。\n2、数据集GO结构数据\n使用GO的有向无环图结构，仅考虑 is_a 和 part_of 两种关系（占88%）。\n涵盖三个子本体：BPO、MFO、CCO。\n\n蛋白质功能预测评估数据\nCAFA3：国际蛋白质功能预测评估数据集。\nSwissProt：高质量标注的蛋白质数据库。\nPFAM 和 PPI 数据集：用于评估GO嵌入与蛋白质功能域、相互作用的关联性。用来额外检验PO2Vec学到的是否真的能反映出蛋白质的功能相似性和相互作用关系。\n\n\n3、实验结果数据、实验结果显示，PO2Vec在GO术语深度预测任务中准确率显著优于基线模型（Anc2Vec、Opa2Vec、TransH等），1-Wasserstein距离达0.41，能清晰分离不同本体领域的术语；在蛋白质功能预测中，PO2GO在CAFA3基准上取得BPO、MFO、CCO的Fmax分别为0.526、0.611、0.648，全面领先现有方法；其预测结果具有更高信息量，在BPO和MFO上的平均信息内容分别为9.46和5.78，且在训练样本稀缺的场景下性能下降最小，展现出强大的泛化与实用价值。\n消融实验移除PO2Vec或替换为多热编码后，性能显著下降。固定PO2Vec嵌入不微调时，性能仍保持良好，说明预训练嵌入质量高。\n\n4、贡献与创新点PO2Vec：首个利用最短可达路径偏序关系进行GO术语嵌入学习的方法。\nBalanced InfoNCE：提出分层采样策略，解决负样本不平衡问题。\nPO2GO：将PO2Vec与ESM-1b结合，实现端到端的蛋白质功能预测。\nPO2Vec方法通过引入最短可达路径的偏序约束和对比学习策略，显著提升了GO术语嵌入的质量。基于PO2Vec构建的PO2GO模型在多个基准数据集上表现出优越的蛋白质功能预测能力，尤其是在预测特异性、少样本学习和层次结构建模方面具有明显优势。该工作为生物医学本体的表示学习与蛋白质功能注释提供了新的思路和技术路径。\n","slug":"PO2GO","date":"2025-10-21T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"6337ea3a45c7ed79f51176d6bcc5ca76","title":"GOHPro","content":"GOHPro蛋白质功能相似性网络构建DomainNet（结构相似性网络）：基于蛋白质域组成（比较两个蛋白质自身拥有的结构域）和邻域上下文构建（比较两个蛋白质的邻居蛋白质所拥有的结构域），结合了上下文相似性和组成相似性，通过参数β平衡（β&#x3D;0.1）。\nModuleNet（模块相似性网络）：基于蛋白质复合物信息（Complex Portal）构建，使用超几何分布评估复合物的功能显著性。\nProteinNet（融合网络）：将 DomainNet 和 ModuleNet 线性融合，权重由参数γ控制（γ依 ontology 不同而调整）。\nGO语义相似性网络构建基于 GO 的层次结构（DAG），仅保留 is_a 和 part_of 关系其他关系可能更复杂或更模糊，保留它们会引入噪声。通过不同的权重计算 GO 术语间的相似性。\n异质网络整合将 ProteinNet、GO-GO 网络、蛋白质-GO 注释网络整合为一个异质网络 GPG。异质网络的价值在于让不同类型的节点通过不同类型的关系相互影响**。一个GO术语的信息可以通过语义关系影响另一个GO术语，同时也可以通过已知注释影响蛋白质，蛋白质再通过功能相似影响其他蛋白质这种跨类型的信息流动让预测更加智能。\n此时，功能未知的蛋白质也在异质网络中，它通过“蛋白-蛋白”边与其他蛋白紧密相连，和GO术语层是断开的。\n\n网络传播算法使用迭代传播公式：WPGt+1&#x3D;α⋅(WP⋅WPGt⋅WG)+(1−α)⋅WPG0，其中 α&#x3D;0.1α&#x3D;0.1，控制信息传播的衰减。所有已知的蛋白质-GO注释（档案）作为信息的起点，信息在 蛋白关系网 中传播：一个已知功能的蛋白，会把它的功能信息传递给与它相似的其他蛋白。信息同时会在 GO术语关系网 中传播：一个GO术语（如“酶活性”）的信息，会传递给与它相似的其他GO术语（如“催化活性”）。信息通过 已知注释 在蛋白层和GO层之间来回流动、相互增强。经过多次迭代，网络中的信息分布不再剧烈变化，达到稳定。最终根据传播后的权重排序，为未知蛋白质推荐 GO 注释。\n\n2、数据集训练和开发数据集：\n使用来自多个权威生物数据库的公开数据构建异质网络。蛋白质-蛋白质相互作用（PPI）数据来自 BioGRID（2024年10月版），包含酵母（3,162个蛋白）和人类（7,317个蛋白）的全物种互作网络。基因本体（GO）注释来自 GO Consortium（2024年9月版），并严格筛选仅保留具有实验证据代码的高质量注释，覆盖 Biological Process (BP), Molecular Function (MF), 和 Cellular Component (CC) 三个本体。蛋白质域信息来自 Pfam 37.0，蛋白质复合物信息来自 Complex Portal（2024年11月版）。在模型评估中，主要采用留一法交叉验证，即将数据集中每个蛋白质依次视为未知功能进行预测，其余蛋白质用于训练，以全面评估模型在已知数据分布下的性能。\n基准测试独立数据集：\nCAFA3基准测试集： 使用国际蛋白质功能注释挑战赛 CAFA3 的官方数据集进行前瞻性评估。使用CAFA3特定时间戳前的蛋白质和注释作为训练集，并将其后新注释的蛋白质作为独立的测试集。例如，对于人类物种，BP、MF、CC本体的训练集规模分别增至5,153、3,109、2,858个蛋白，而测试集规模分别为188、554、642个蛋白。该测试集模拟了预测全新发现蛋白质功能的真实场景，有效评估模型的泛化能力。\n案例验证数据集：\n共享结构域蛋白集： 精选4个共享AAA+ATPase结构域但功能各异的蛋白质（如人源PEX6、VCP；酵母ORC1、YTA12），用于验证模型在区分功能细微差别和解决功能歧义方面的能力。\n网络扰动测试集： 通过对上述案例蛋白系统地删除其关键结构域或相互作用，构建扰动数据集，用于分析模型预测结果对关键生物学特征的依赖性和鲁棒性。\n\n3、实验结果数据整体性能对比：\n在酵母和人类数据集上，与六种先进工具（exp2GO、GrAPFI-GO、PHN、DCS、NC、DeepGO-SE）进行比较，GOHPro在主要评估指标Fmax上展现出全面优势。\n酵母数据集： GOHPro 在 Biological Process (BP), Molecular Function (MF), 和 Cellular Component (CC) 上的 Fmax 分别达到 0.45, 0.548, 0.58。\n人类数据集： GOHPro 在 BP&#x2F;MF&#x2F;CC 上的 Fmax 分别为 0.261, 0.447, 0.354，其中在 Molecular Function (MF) 上的性能提升最为显著，相较次优方法提升幅度达 **47.5%**。\nCAFA3独立基准测试： 在更具挑战性的CAFA3基准上，GOHPro的泛化能力得到验证，在人类物种上的 Fmax 提升超过 **62%**。\n功能特异性与鲁棒性分析：\n共享结构域蛋白功能区分： 在四个共享AAA+ATPase结构域但功能各异的蛋白质案例中，GOHPro能准确预测出其不同的生物学功能，成功解决了功能歧义问题。\n敏感性分析： 实验表明，当去除蛋白质的关键相互作用或复合物成员信息时，GOHPro对相应功能的预测精度显著下降。\n低同源性与稀疏网络下的性能：\n低同源性（Dark）蛋白质表现： 对于同源性低（&lt;40%）、进化信息稀缺的“暗蛋白质”，GOHPro通过整合模块相似性网络，仍能提供有效的功能推断。DomainNet在其中发挥了关键的补充作用。\n网络连通性敏感性： 网络稀疏性实验显示，低同源性蛋白质对网络连接的完整性更为敏感。当90%的网络边被移除时，其Fmax性能下降高达76%，显著高于高同源性蛋白质的33% 下降幅度，凸显了其依赖密集网络进行信息传递的特性。\n\n4、贡献与创新点提出异质网络融合策略，整合了：蛋白质功能相似性+GO 语义相似性+已知蛋白质-GO 注释\n引入网络传播算法，实现全局信息扩散，缓解数据稀疏性问题。\n揭示了同源性和网络连通性对预测性能的影响，为 dark protein 注释提供新思路。\n在酵母和人类数据集上系统评估，涵盖多个评估指标。\nGOHPro 是一种基于异质网络传播的蛋白质功能预测方法，通过融合多源数据（PPI、域、复合物、GO语义）构建统一网络，并利用传播算法实现功能注释的全局优化。在酵母和人类数据集上，GOHPro 在多个评估指标上显著优于现有方法，尤其在区分共享结构域蛋白质的功能方面表现出色。其模块化设计和可扩展性为未来整合更多数据类型（如结构信息）奠定了基础，是蛋白质功能预测领域一项重要进展。\n","slug":"GOHPro","date":"2025-10-20T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"a51592e2b5599f8db32fcf648d257b16","title":"ProstaNet","content":"ProstaNet一.研究背景\n天然的蛋白质有时不够稳定\n实验方法测定突变对稳定性的影响成本高、耗时长。\n现有计算方法分为基于物理模型和基于机器学习的方法，但存在数据偏差、结构表示不全面、对多点突变预测能力弱等问题。\n\n二.研究问题\n如何提高蛋白质稳定性预测的准确性，尤其是对于多点突变？\n如何解决当前数据量不足、数据偏差大、缺乏多点突变基准测试集的问题？\n如何有效结合几何信息与关系信息来提升模型性能？\n\n三.研究目的开发一个高精度的蛋白质热稳定性预测模型，能够同时处理单点突变和多点突变。\n四.研究方法\n数据构建：\n从公共数据集上收集原始数据，去噪后构建ProstaDB，包含3,784个单点突变和1,642个多点突变。\n提出热力学循环 数据增强方法，像高中学的热化学反应方程式，能从已有的数据里推理和创造出新的、合理的数据，扩充多点突变数据至7,360条。\n使用反称性增强 ，利用数据的对称性来平衡正负样本，让模型的预测没有偏见。\n\n\n模型设计：\n用GVP-GNN 提取结构图中的几何与关系特征，能够更全面、更精确地“感受”到蛋白质结构的微妙变化，从而在预测突变影响时，比只关注一种信息的模型准确得多。而且对于每个节点，会从邻居处获取信息，这个信息包含节点特征和边特征，在空间中还会保持等变性，即无论是旋转还是平移，这些特征都会跟着变化。\n节点特征（描述一个氨基酸“自身”）：\n标量特征 S_v：描述氨基酸的内在属性。比如：它是哪种氨基酸（用One-hot编码）、它的疏水性得分、它的进化保守性（PSSM）等。这些是没有方向的信息。\n向量特征 V_v：描述氨基酸在空间中的朝向和方向。具体来说，就是其主链上的化学键方向（如Cα-C键的方向、Cα-N键的方向）。这些是有方向的信息，就像一个小箭头。\n\n\n边特征（描述两个氨基酸之间的“关系”）：\n标量特征 S_e：描述关系的强度或距离。最直接的就是两个氨基酸Cα原子之间的欧氏距离。\n向量特征 V_e：描述两个氨基酸之间的相对位置和方向。通常就是从一个原子指向另一个原子的方向向量。\n\n\n结合多头注意力机制 比较野生型与突变型之间的差异。\n预测结果从具体的ΔΔG值，转化为稳定与否的二元问题。\n采用预训练（单点）→ 微调（多点） 的训练策略，用大数据（单点突变）预训练，用小数据（多点突变）微调。让模型学习当多个单点突变同时发生时，它们之间是如何相互影响的，是否会有1+1&gt;2？\n\n\n特征编码：\n评估7类氨基酸编码方法（如残基打分、进化特征、物理化学属性等），发现残基打分对预测性能影响最大。\n\n\n聚类方法：使用t-SNE + DBSCAN 对突变类型聚类，将不同的多点突变组合按类型分组，确保训练集与测试集的突变类型组合分布一致。（类似分层抽样）\n\n五.分析方法\n消融实验：评估不同编码方法对模型性能的贡献。\n对比实验：与ThermoMPNN、FoldX、PoPMuSiC等9种现有方法比较。\n实验验证：通过圆二色谱验证HuJ3纳米抗体突变的稳定性预测结果。\n\n六.创新点\n首次将GVP-GNN应用于蛋白质稳定性预测，结合几何与关系特征。\n构建高质量数据库ProstaDB，涵盖单点与多点突变，并提出TL数据增强方法。\n提出聚类方法构建多点突变测试集，解决基准缺失问题。\n系统评估氨基酸编码方法，明确残基打分和PSSM在多点突变中的关键作用。\n实验验证：通过HuJ3纳米抗体突变实验验证模型实用性。\n\n七.局限性\n数据限制：训练数据量仍不足，尤其是单点突变缺乏有效的数据增强方法。\n结构依赖：模型依赖晶体结构，而实际中多使用AlphaFold2预测结构，存在偏差。\n突变结构建模：Rosetta Fast Relax无法模拟大构象变化，影响突变结构准确性。\n\n","slug":"ProstaNet (1)","date":"2025-10-15T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"99cc996dc0617c9203fb12b8cb30ba1d","title":"Kaggle房价预测","content":"Kaggle房价预测12345678# 若无法获得测试数据，则可根据训练数据计算均值和标准差numeric_features = all_features.dtypes[all_features.dtypes != &#x27;object&#x27;].index# 对每个数字特征进行标准化，把数据变成以0为中心，标准差为1的分布# 数据围绕着0分布，可以将不同尺度的特征放在一起比较，防止差异性带来的权重不平衡问题all_features[numeric_features] = all_features[numeric_features].apply(    lambda x: (x - x.mean()) / (x.std()))# 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0all_features[numeric_features] = all_features[numeric_features].fillna(0)\n\n\n查看每个特征的数据类型，为后续数据预处理做准备\n\n123456# &quot;Dummy_na=True&quot;将&quot;na&quot;（缺失值）视为有效的特征值，并为其创建指示符特征all_features = pd.get_dummies(all_features, dummy_na=True)#将类别特征转换为数值形式（独热编码）#独热编码可以消除数学大小关系，遇到任何新的类别值，就给所有数据创建对应的列#但会导致特征爆炸性增长all_features = pd.get_dummies(all_features, dummy_na=True)\n\n\n1234567#就是一个线性模型，每个参数的权重不一样loss = nn.MSELoss()in_features = train_features.shape[1]def get_net():    net = nn.Sequential(nn.Linear(in_features,1))    return net\n\n1234567def log_rmse(net, features, labels):    # 为了在取对数时进一步稳定该值，将小于1的值设置为1    clipped_preds = torch.clamp(net(features), 1, float(&#x27;inf&#x27;))    rmse = torch.sqrt(loss(torch.log(clipped_preds),                           torch.log(labels)))    return rmse.item()# 不看绝对数量，看相对比例\n\n1234567891011121314151617181920def train(net, train_features, train_labels, test_features, test_labels,          num_epochs, learning_rate, weight_decay, batch_size):    train_ls, test_ls = [], []    train_iter = d2l.load_array((train_features, train_labels), batch_size)    # 这里使用的是Adam优化算法,还可以防止过拟合    optimizer = torch.optim.Adam(net.parameters(),                                 lr = learning_rate,                                 weight_decay = weight_decay)    for epoch in range(num_epochs):        for X, y in train_iter:            #训练的老套路            optimizer.zero_grad()            l = loss(net(X), y)            l.backward()            optimizer.step()        train_ls.append(log_rmse(net, train_features, train_labels))        if test_labels is not None:            test_ls.append(log_rmse(net, test_features, test_labels))    return train_ls, test_ls#返回训练结果\n\n12345678910111213141516171819202122232425262728293031323334def get_k_fold_data(k, i, X, y):    assert k &gt; 1    fold_size = X.shape[0] // k    X_train, y_train = None, None    for j in range(k):        idx = slice(j * fold_size, (j + 1) * fold_size)        X_part, y_part = X[idx, :], y[idx]        if j == i:            X_valid, y_valid = X_part, y_part        elif X_train is None:            X_train, y_train = X_part, y_part        else:            X_train = torch.cat([X_train, X_part], 0)            y_train = torch.cat([y_train, y_part], 0)    return X_train, y_train, X_valid, y_validdef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,           batch_size):    train_l_sum, valid_l_sum = 0, 0    for i in range(k):        data = get_k_fold_data(k, i, X_train, y_train)        net = get_net()        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,                                   weight_decay, batch_size)        train_l_sum += train_ls[-1]        valid_l_sum += valid_ls[-1]        if i == 0:            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],                     xlabel=&#x27;epoch&#x27;, ylabel=&#x27;rmse&#x27;, xlim=[1, num_epochs],                     legend=[&#x27;train&#x27;, &#x27;valid&#x27;], yscale=&#x27;log&#x27;)        print(f&#x27;折&#123;i + 1&#125;，训练log rmse&#123;float(train_ls[-1]):f&#125;, &#x27;              f&#x27;验证log rmse&#123;float(valid_ls[-1]):f&#125;&#x27;)    return train_l_sum / k, valid_l_sum / k#K折训练算法：将训练集人为的划分成几份，利用其中一份进行验证，剩余的拿来训练。依次充当验证集，最后取平均值，既充分利用了数据，又得到了最公平的模型评估。\n\n12345k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,                          weight_decay, batch_size)print(f&#x27;&#123;k&#125;-折验证: 平均训练log rmse: &#123;float(train_l):f&#125;, &#x27;      f&#x27;平均验证log rmse: &#123;float(valid_l):f&#125;&#x27;)\n\n\n","slug":"Kaggle房价预测","date":"2025-10-13T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"},{"id":"0ef6094797649f91ccbc2e9b364eb919","title":"序列数据","content":"序列数据12345678T = 1000  # 总共产生1000个点time = d2l.arange(1, T + 1, dtype=d2l.float32)#定义时间序列的长度（横轴）为1000个点x = d2l.sin(0.01 * time) + d2l.normal(0, 0.2, (T,))#生成正弦波并且添加高斯噪声d2l.plot(time, [x], &#x27;time&#x27;, &#x27;x&#x27;, xlim=[1, 1000], figsize=(6, 3))#x轴数据、y轴数据、x轴标签、y轴标签、x轴显示范围、图表尺寸\n\n\n1234epoch：训练的完整周期batch/mini-batch：分批训练Iterations（迭代次数）也就是batch的数量batchSize：每个batch包含的样本数量\n\n1234567891011121314tau = 4features = d2l.zeros((T - tau, tau))#特征为996行，4列for i in range(tau):    features[:, i] = x[i: T - tau + i]#初始化特征矩阵labels = d2l.reshape(x[tau:], (-1, 1))#重塑一下标签矩阵#@tab allbatch_size, n_train = 16, 600# 只有前n_train个样本用于训练train_iter = d2l.load_array((features[:n_train], labels[:n_train]),                            batch_size, is_train=True)#创建好数据加载器，用于模型训练，用前600个样本，每次训练使用16个样本\n\n12345678910111213141516# 初始化网络权重的函数#如果是线性层，就要用Xavier方法初始化权重，自动计算合适的初始化范围def init_weights(m):    if type(m) == nn.Linear:        nn.init.xavier_uniform_(m.weight)# 一个简单的多层感知机def get_net():    net = nn.Sequential(nn.Linear(4, 10),#输入4个特征，输出10个神经元                        nn.ReLU(),#激活函数，非线性                        nn.Linear(10, 1))#通过10个神经元，输出一个预测值    net.apply(init_weights)#给所有层应用权重初始化    return net# 平方损失。注意：MSELoss计算平方误差时不带系数1/2loss = nn.MSELoss(reduction=&#x27;none&#x27;)\n\n123456789101112131415def train(net, train_iter, loss, epochs, lr):    trainer = torch.optim.Adam(net.parameters(), lr)    #这是一个高度封装的的训练器，只需要传进去网络参数以及学习率就可以    for epoch in range(epochs):        for X, y in train_iter:            #训练的套路基本都是固定的            trainer.zero_grad()            l = loss(net(X), y)            l.sum().backward()            trainer.step()        print(f&#x27;epoch &#123;epoch + 1&#125;, &#x27;              f&#x27;loss: &#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;&#x27;)net = get_net()train(net, train_iter, loss, 5, 0.01)\n\n\n1234567onestep_preds = net(features)#调用的是net.__call__(features)#得到996个预测值d2l.plot([time, time[tau:]],         [d2l.numpy(x), d2l.numpy(onestep_preds)], &#x27;time&#x27;,         &#x27;x&#x27;, legend=[&#x27;data&#x27;, &#x27;1-step preds&#x27;], xlim=[1, 1000],         figsize=(6, 3))\n\n\n123456789101112131415multistep_preds = d2l.zeros(T)multistep_preds[: n_train + tau] = x[: n_train + tau]#前面的604个值都用真实数据填充for i in range(n_train + tau, T):    multistep_preds[i] = net(        d2l.reshape(multistep_preds[i - tau: i], (1, -1)))    #从第605开始，到1000，就开始用前四个来计算当前的值    #误差会累积，所以越来越偏离#@tab alld2l.plot([time, time[tau:], time[n_train + tau:]],         #原始数据、从第五个点开始：单步预测、从第605个点开始：多步预测         [d2l.numpy(x), d2l.numpy(onestep_preds),          d2l.numpy(multistep_preds[n_train + tau:])], &#x27;time&#x27;,         &#x27;x&#x27;, legend=[&#x27;data&#x27;, &#x27;1-step preds&#x27;, &#x27;multistep preds&#x27;],         xlim=[1, 1000], figsize=(6, 3))\n\n\n123456789101112131415161718#@tab allmax_steps = 64#@tab mxnet, pytorchfeatures = d2l.zeros((T - tau - max_steps + 1, tau + max_steps))# 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）for i in range(tau):    features[:, i] = x[i: i + T - tau - max_steps + 1]# 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）for i in range(tau, tau + max_steps):    features[:, i] = d2l.reshape(net(features[:, i - tau: i]), -1)#%%#只选择了4个有代表性的步长steps = (1, 4, 16, 64)d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],         [d2l.numpy(features[:, tau + i - 1]) for i in steps], &#x27;time&#x27;, &#x27;x&#x27;,         legend=[f&#x27;&#123;i&#125;-step preds&#x27; for i in steps], xlim=[5, 1000],         figsize=(6, 3))\n\n\n","slug":"序列数据","date":"2025-10-11T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"},{"id":"dd8e371b161018fe37f17369405b9122","title":"深度学习笔记","content":"深度学习\n梯度下降算法：用来优化参数，让模型的预测更准确\n\n1.1  计算梯度\n1.2  更新参数（需要设置好学习率，就是步长）\n1.3  不断重复，直到梯度为零\n新的参数 &#x3D; 旧的参数 - 学习率 × (损失函数在旧参数点的梯度)\n\n最终目标是要使损失函数最小\n训练过程：\n\n3.1 前向传播\n3.2 计算损失（交叉熵损失）\n3.3 反向传播\n3.4 参数更新\n3.5 循环迭代\n\n激活函数：引入非线性（不同任务类型要用不同的激活函数）\nCNN\n\n输入-卷积-激活函数（简单特征到复杂特征的非线性映射）-池化（简化特征）-展平-全连接-输出\n","slug":"深度学习笔记","date":"2025-09-27T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"}]