[{"id":"a51592e2b5599f8db32fcf648d257b16","title":"ProstaNet","content":"ProstaNet一.研究背景\n天然的蛋白质有时不够稳定\n实验方法测定突变对稳定性的影响成本高、耗时长。\n现有计算方法分为基于物理模型和基于机器学习的方法，但存在数据偏差、结构表示不全面、对多点突变预测能力弱等问题。\n\n二.研究问题\n如何提高蛋白质稳定性预测的准确性，尤其是对于多点突变？\n如何解决当前数据量不足、数据偏差大、缺乏多点突变基准测试集的问题？\n如何有效结合几何信息与关系信息来提升模型性能？\n\n三.研究目的开发一个高精度的蛋白质热稳定性预测模型，能够同时处理单点突变和多点突变。\n四.研究方法\n数据构建：\n从公共数据集上收集原始数据，去噪后构建ProstaDB，包含3,784个单点突变和1,642个多点突变。\n提出热力学循环 数据增强方法，像高中学的热化学反应方程式，能从已有的数据里推理和创造出新的、合理的数据，扩充多点突变数据至7,360条。\n使用反称性增强 ，利用数据的对称性来平衡正负样本，让模型的预测没有偏见。\n\n\n模型设计：\n用GVP-GNN 提取结构图中的几何与关系特征，能够更全面、更精确地“感受”到蛋白质结构的微妙变化，从而在预测突变影响时，比只关注一种信息的模型准确得多。而且对于每个节点，会从邻居处获取信息，这个信息包含节点特征和边特征，在空间中还会保持等变性，即无论是旋转还是平移，这些特征都会跟着变化。\n节点特征（描述一个氨基酸“自身”）：\n标量特征 S_v：描述氨基酸的内在属性。比如：它是哪种氨基酸（用One-hot编码）、它的疏水性得分、它的进化保守性（PSSM）等。这些是没有方向的信息。\n向量特征 V_v：描述氨基酸在空间中的朝向和方向。具体来说，就是其主链上的化学键方向（如Cα-C键的方向、Cα-N键的方向）。这些是有方向的信息，就像一个小箭头。\n\n\n边特征（描述两个氨基酸之间的“关系”）：\n标量特征 S_e：描述关系的强度或距离。最直接的就是两个氨基酸Cα原子之间的欧氏距离。\n向量特征 V_e：描述两个氨基酸之间的相对位置和方向。通常就是从一个原子指向另一个原子的方向向量。\n\n\n结合多头注意力机制 比较野生型与突变型之间的差异。\n预测结果从具体的ΔΔG值，转化为稳定与否的二元问题。\n采用预训练（单点）→ 微调（多点） 的训练策略，用大数据（单点突变）预训练，用小数据（多点突变）微调。让模型学习当多个单点突变同时发生时，它们之间是如何相互影响的，是否会有1+1&gt;2？\n\n\n特征编码：\n评估7类氨基酸编码方法（如残基打分、进化特征、物理化学属性等），发现残基打分对预测性能影响最大。\n\n\n聚类方法：使用t-SNE + DBSCAN 对突变类型聚类，将不同的多点突变组合按类型分组，确保训练集与测试集的突变类型组合分布一致。（类似分层抽样）\n\n五.分析方法\n消融实验：评估不同编码方法对模型性能的贡献。\n对比实验：与ThermoMPNN、FoldX、PoPMuSiC等9种现有方法比较。\n实验验证：通过圆二色谱验证HuJ3纳米抗体突变的稳定性预测结果。\n\n六.创新点\n首次将GVP-GNN应用于蛋白质稳定性预测，结合几何与关系特征。\n构建高质量数据库ProstaDB，涵盖单点与多点突变，并提出TL数据增强方法。\n提出聚类方法构建多点突变测试集，解决基准缺失问题。\n系统评估氨基酸编码方法，明确残基打分和PSSM在多点突变中的关键作用。\n实验验证：通过HuJ3纳米抗体突变实验验证模型实用性。\n\n七.局限性\n数据限制：训练数据量仍不足，尤其是单点突变缺乏有效的数据增强方法。\n结构依赖：模型依赖晶体结构，而实际中多使用AlphaFold2预测结构，存在偏差。\n突变结构建模：Rosetta Fast Relax无法模拟大构象变化，影响突变结构准确性。\n\n","slug":"ProstaNet (1)","date":"2025-10-15T16:00:00.000Z","categories_index":"组会","tags_index":"文献","author_index":"Qushubiao"},{"id":"99cc996dc0617c9203fb12b8cb30ba1d","title":"Kaggle房价预测","content":"Kaggle房价预测12345678# 若无法获得测试数据，则可根据训练数据计算均值和标准差numeric_features = all_features.dtypes[all_features.dtypes != &#x27;object&#x27;].index# 对每个数字特征进行标准化，把数据变成以0为中心，标准差为1的分布# 数据围绕着0分布，可以将不同尺度的特征放在一起比较，防止差异性带来的权重不平衡问题all_features[numeric_features] = all_features[numeric_features].apply(    lambda x: (x - x.mean()) / (x.std()))# 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0all_features[numeric_features] = all_features[numeric_features].fillna(0)\n\n\n查看每个特征的数据类型，为后续数据预处理做准备\n\n123456# &quot;Dummy_na=True&quot;将&quot;na&quot;（缺失值）视为有效的特征值，并为其创建指示符特征all_features = pd.get_dummies(all_features, dummy_na=True)#将类别特征转换为数值形式（独热编码）#独热编码可以消除数学大小关系，遇到任何新的类别值，就给所有数据创建对应的列#但会导致特征爆炸性增长all_features = pd.get_dummies(all_features, dummy_na=True)\n\n\n1234567#就是一个线性模型，每个参数的权重不一样loss = nn.MSELoss()in_features = train_features.shape[1]def get_net():    net = nn.Sequential(nn.Linear(in_features,1))    return net\n\n1234567def log_rmse(net, features, labels):    # 为了在取对数时进一步稳定该值，将小于1的值设置为1    clipped_preds = torch.clamp(net(features), 1, float(&#x27;inf&#x27;))    rmse = torch.sqrt(loss(torch.log(clipped_preds),                           torch.log(labels)))    return rmse.item()# 不看绝对数量，看相对比例\n\n1234567891011121314151617181920def train(net, train_features, train_labels, test_features, test_labels,          num_epochs, learning_rate, weight_decay, batch_size):    train_ls, test_ls = [], []    train_iter = d2l.load_array((train_features, train_labels), batch_size)    # 这里使用的是Adam优化算法,还可以防止过拟合    optimizer = torch.optim.Adam(net.parameters(),                                 lr = learning_rate,                                 weight_decay = weight_decay)    for epoch in range(num_epochs):        for X, y in train_iter:            #训练的老套路            optimizer.zero_grad()            l = loss(net(X), y)            l.backward()            optimizer.step()        train_ls.append(log_rmse(net, train_features, train_labels))        if test_labels is not None:            test_ls.append(log_rmse(net, test_features, test_labels))    return train_ls, test_ls#返回训练结果\n\n12345678910111213141516171819202122232425262728293031323334def get_k_fold_data(k, i, X, y):    assert k &gt; 1    fold_size = X.shape[0] // k    X_train, y_train = None, None    for j in range(k):        idx = slice(j * fold_size, (j + 1) * fold_size)        X_part, y_part = X[idx, :], y[idx]        if j == i:            X_valid, y_valid = X_part, y_part        elif X_train is None:            X_train, y_train = X_part, y_part        else:            X_train = torch.cat([X_train, X_part], 0)            y_train = torch.cat([y_train, y_part], 0)    return X_train, y_train, X_valid, y_validdef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,           batch_size):    train_l_sum, valid_l_sum = 0, 0    for i in range(k):        data = get_k_fold_data(k, i, X_train, y_train)        net = get_net()        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,                                   weight_decay, batch_size)        train_l_sum += train_ls[-1]        valid_l_sum += valid_ls[-1]        if i == 0:            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],                     xlabel=&#x27;epoch&#x27;, ylabel=&#x27;rmse&#x27;, xlim=[1, num_epochs],                     legend=[&#x27;train&#x27;, &#x27;valid&#x27;], yscale=&#x27;log&#x27;)        print(f&#x27;折&#123;i + 1&#125;，训练log rmse&#123;float(train_ls[-1]):f&#125;, &#x27;              f&#x27;验证log rmse&#123;float(valid_ls[-1]):f&#125;&#x27;)    return train_l_sum / k, valid_l_sum / k#K折训练算法：将训练集人为的划分成几份，利用其中一份进行验证，剩余的拿来训练。依次充当验证集，最后取平均值，既充分利用了数据，又得到了最公平的模型评估。\n\n12345k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,                          weight_decay, batch_size)print(f&#x27;&#123;k&#125;-折验证: 平均训练log rmse: &#123;float(train_l):f&#125;, &#x27;      f&#x27;平均验证log rmse: &#123;float(valid_l):f&#125;&#x27;)\n\n\n","slug":"Kaggle房价预测","date":"2025-10-13T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"},{"id":"0ef6094797649f91ccbc2e9b364eb919","title":"序列数据","content":"序列数据12345678T = 1000  # 总共产生1000个点time = d2l.arange(1, T + 1, dtype=d2l.float32)#定义时间序列的长度（横轴）为1000个点x = d2l.sin(0.01 * time) + d2l.normal(0, 0.2, (T,))#生成正弦波并且添加高斯噪声d2l.plot(time, [x], &#x27;time&#x27;, &#x27;x&#x27;, xlim=[1, 1000], figsize=(6, 3))#x轴数据、y轴数据、x轴标签、y轴标签、x轴显示范围、图表尺寸\n\n\n1234epoch：训练的完整周期batch/mini-batch：分批训练Iterations（迭代次数）也就是batch的数量batchSize：每个batch包含的样本数量\n\n1234567891011121314tau = 4features = d2l.zeros((T - tau, tau))#特征为996行，4列for i in range(tau):    features[:, i] = x[i: T - tau + i]#初始化特征矩阵labels = d2l.reshape(x[tau:], (-1, 1))#重塑一下标签矩阵#@tab allbatch_size, n_train = 16, 600# 只有前n_train个样本用于训练train_iter = d2l.load_array((features[:n_train], labels[:n_train]),                            batch_size, is_train=True)#创建好数据加载器，用于模型训练，用前600个样本，每次训练使用16个样本\n\n12345678910111213141516# 初始化网络权重的函数#如果是线性层，就要用Xavier方法初始化权重，自动计算合适的初始化范围def init_weights(m):    if type(m) == nn.Linear:        nn.init.xavier_uniform_(m.weight)# 一个简单的多层感知机def get_net():    net = nn.Sequential(nn.Linear(4, 10),#输入4个特征，输出10个神经元                        nn.ReLU(),#激活函数，非线性                        nn.Linear(10, 1))#通过10个神经元，输出一个预测值    net.apply(init_weights)#给所有层应用权重初始化    return net# 平方损失。注意：MSELoss计算平方误差时不带系数1/2loss = nn.MSELoss(reduction=&#x27;none&#x27;)\n\n123456789101112131415def train(net, train_iter, loss, epochs, lr):    trainer = torch.optim.Adam(net.parameters(), lr)    #这是一个高度封装的的训练器，只需要传进去网络参数以及学习率就可以    for epoch in range(epochs):        for X, y in train_iter:            #训练的套路基本都是固定的            trainer.zero_grad()            l = loss(net(X), y)            l.sum().backward()            trainer.step()        print(f&#x27;epoch &#123;epoch + 1&#125;, &#x27;              f&#x27;loss: &#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;&#x27;)net = get_net()train(net, train_iter, loss, 5, 0.01)\n\n\n1234567onestep_preds = net(features)#调用的是net.__call__(features)#得到996个预测值d2l.plot([time, time[tau:]],         [d2l.numpy(x), d2l.numpy(onestep_preds)], &#x27;time&#x27;,         &#x27;x&#x27;, legend=[&#x27;data&#x27;, &#x27;1-step preds&#x27;], xlim=[1, 1000],         figsize=(6, 3))\n\n\n123456789101112131415multistep_preds = d2l.zeros(T)multistep_preds[: n_train + tau] = x[: n_train + tau]#前面的604个值都用真实数据填充for i in range(n_train + tau, T):    multistep_preds[i] = net(        d2l.reshape(multistep_preds[i - tau: i], (1, -1)))    #从第605开始，到1000，就开始用前四个来计算当前的值    #误差会累积，所以越来越偏离#@tab alld2l.plot([time, time[tau:], time[n_train + tau:]],         #原始数据、从第五个点开始：单步预测、从第605个点开始：多步预测         [d2l.numpy(x), d2l.numpy(onestep_preds),          d2l.numpy(multistep_preds[n_train + tau:])], &#x27;time&#x27;,         &#x27;x&#x27;, legend=[&#x27;data&#x27;, &#x27;1-step preds&#x27;, &#x27;multistep preds&#x27;],         xlim=[1, 1000], figsize=(6, 3))\n\n\n123456789101112131415161718#@tab allmax_steps = 64#@tab mxnet, pytorchfeatures = d2l.zeros((T - tau - max_steps + 1, tau + max_steps))# 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）for i in range(tau):    features[:, i] = x[i: i + T - tau - max_steps + 1]# 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）for i in range(tau, tau + max_steps):    features[:, i] = d2l.reshape(net(features[:, i - tau: i]), -1)#%%#只选择了4个有代表性的步长steps = (1, 4, 16, 64)d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],         [d2l.numpy(features[:, tau + i - 1]) for i in steps], &#x27;time&#x27;, &#x27;x&#x27;,         legend=[f&#x27;&#123;i&#125;-step preds&#x27; for i in steps], xlim=[5, 1000],         figsize=(6, 3))\n\n\n","slug":"序列数据","date":"2025-10-11T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"},{"id":"dd8e371b161018fe37f17369405b9122","title":"深度学习笔记","content":"深度学习\n梯度下降算法：用来优化参数，让模型的预测更准确\n\n1.1  计算梯度\n1.2  更新参数（需要设置好学习率，就是步长）\n1.3  不断重复，直到梯度为零\n新的参数 &#x3D; 旧的参数 - 学习率 × (损失函数在旧参数点的梯度)\n\n最终目标是要使损失函数最小\n训练过程：\n\n3.1 前向传播\n3.2 计算损失（交叉熵损失）\n3.3 反向传播\n3.4 参数更新\n3.5 循环迭代\n\n激活函数：引入非线性（不同任务类型要用不同的激活函数）\nCNN\n\n输入-卷积-激活函数（简单特征到复杂特征的非线性映射）-池化（简化特征）-展平-全连接-输出\n","slug":"深度学习笔记","date":"2025-09-27T16:00:00.000Z","categories_index":"技术","tags_index":"深度学习","author_index":"Qushubiao"}]