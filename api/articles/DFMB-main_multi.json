{"title":"DFMB-main_multis","uid":"c8138eb84d60350b55d7d9a3f5fb8c3f","slug":"DFMB-main_multi","date":"2025-12-05T16:00:00.000Z","updated":"2025-12-07T11:20:42.696Z","comments":true,"path":"api/articles/DFMB-main_multi.json","keywords":null,"cover":[],"content":"<h2 id=\"明确概念及用途\"><a href=\"#明确概念及用途\" class=\"headerlink\" title=\"明确概念及用途\"></a>明确概念及用途</h2><p>net_pid_list：读取网络中所有蛋白质 ID 。（如 P12345）。</p>\n<p>net_pid_map：ppi网络，也是将蛋白质 ID 翻译成图网络中的节点编号。例如 {‘P12345’: 0, ‘Q99999’: 1}。</p>\n<p>net_blastdb：读取 BLAST 数据库的路径，用于后续处理那些不在这个列表里的“孤儿蛋白”。</p>\n<p>network_x：这是基于 <strong>InterProScan</strong> 工具提取的蛋白质<strong>功能域、家族、Motif</strong> 信息 。</p>\n<p>network_esm:这是利用预训练的大规模<strong>蛋白质语言模型（ESM-1b）</strong> 对蛋白质序列进行编码得到的<strong>进化特征</strong> 。</p>\n<p>train_y：train对应的真实的go标签。</p>\n<p>ppi_train_idx：全局ID到局部索引的转换。</p>\n<h2 id=\"加载双图结构\"><a href=\"#加载双图结构\" class=\"headerlink\" title=\"加载双图结构\"></a>加载双图结构</h2><p><code>dgl.load_graphs</code> 这个函数返回的数据结构比较特殊，它返回的是一个<strong>元组 (Tuple)<strong>，包含两部分：<code>(图列表, 标签字典)</code>。代码中的第一个 <code>[0]</code>： <code>dgl.load_graphs(...)[0]</code> 意思就是：</strong>“只要那个图列表，不要后面的标签字典。”</strong>虽然这个文件里可能只存了一张图，但在 DGL 的格式里它依然被包裹在一个列表里。 所以，代码中的第二个 <code>[0]</code>： <code>(...)[0][0]</code> 意思就是：<strong>“从这个图列表中，取出第一个（也是唯一一个）图对象。”</strong></p>\n<p>这行代码 <code>dgl.load_graphs(...)[0][0]</code> 的完整翻译是：</p>\n<ol>\n<li><strong>加载文件</strong>，得到 <code>(图列表, 标签)</code>。</li>\n<li><strong>取第 0 个元素</strong>，得到 <code>图列表</code>。</li>\n<li><strong>再取列表的第 0 个元素</strong>，得到最终需要的 <strong><code>DGLGraph</code> 对象</strong>。</li>\n</ol>\n<h2 id=\"自环边用来实现残差连接\"><a href=\"#自环边用来实现残差连接\" class=\"headerlink\" title=\"自环边用来实现残差连接\"></a>自环边用来实现残差连接</h2><p>这一步是为了让图神经网络（GCN）能够区分“哪些信息来自邻居”和“哪些信息来自节点自己”。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">self_loop = torch.zeros_like(dgl_graph.edata[<span class=\"string\">&#x27;ppi&#x27;</span>])</span><br><span class=\"line\">    <span class=\"comment\">#先造一列全是 0 的数据（长度等于边的总数）。</span></span><br><span class=\"line\">    self_loop[dgl_graph.edge_ids(nr_:=np.arange(dgl_graph.number_of_nodes()), nr_)] = <span class=\"number\">1.0</span></span><br><span class=\"line\">    <span class=\"comment\">#nr_:=np.arange(...)：生成一个包含所有节点 ID 的数组 [0, 1, 2, ..., N]。</span></span><br><span class=\"line\">    <span class=\"comment\">#找到所有“起点=终点”的行（自环边），把这列数据里对应位置的值改成 1.0。</span></span><br><span class=\"line\">    dgl_graph.edata[<span class=\"string\">&#x27;self&#x27;</span>] = self_loop</span><br><span class=\"line\">    <span class=\"comment\">#把这一列新做好的数据，贴到原图上，命名为 &#x27;self&#x27;。</span></span><br><span class=\"line\">    <span class=\"comment\">#模型在需要的时候，会分别调用这两套权重进行计算，看是需要ppi还是保留自身特征。</span></span><br><span class=\"line\">    <span class=\"comment\">#边 ID\t起点\t终点\t&#x27;ppi&#x27; (原有权重)\t&#x27;self&#x27; (新增权重)</span></span><br><span class=\"line\">    <span class=\"comment\">#0\t    蛋白A\t蛋白B\t 0.95\t             0.0</span></span><br><span class=\"line\">    <span class=\"comment\">#2\t    蛋白A\t蛋白A\t 1.0\t             1.0</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"闭集假设和标签二值化\"><a href=\"#闭集假设和标签二值化\" class=\"headerlink\" title=\"闭集假设和标签二值化\"></a>闭集假设和标签二值化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mlb = get_mlb(Path(data_cnf[<span class=\"string\">&#x27;mlb&#x27;</span>]), train_go)</span><br><span class=\"line\">   <span class=\"comment\">#程序扫描整个训练集，把出现过的所有功能整理成一个固定的列表，并将它们排序建立索引，比如 1000 个功能）。</span></span><br><span class=\"line\">   <span class=\"comment\">#神经网络的输出层（Output Layer） 就被物理地设计成了 1000 个神经元。第 1 个神经元对应列表里的第 1 个功能，第 1000 个对应第 1000 个。</span></span><br><span class=\"line\">   <span class=\"comment\">#Fit（拟合）：这一步只针对 train_go 进行。这意味着模型只学习训练集中见过的标签</span></span><br><span class=\"line\">   <span class=\"comment\"># 如果验证集/测试集中出现了全新的 GO Term，通常会被忽略。</span></span><br><span class=\"line\">   labels_num = <span class=\"built_in\">len</span>(mlb.classes_)</span><br><span class=\"line\">   <span class=\"comment\">#统计总共有多少个唯一的 GO Term</span></span><br><span class=\"line\">   <span class=\"comment\">#在构建深度学习模型时，输出层（Output Layer）的神经元个数必须固定。</span></span><br><span class=\"line\">   <span class=\"comment\"># 模型并不知道具体的 GO ID 是什么，它只负责输出一组概率值。</span></span><br><span class=\"line\">   <span class=\"comment\">#如果 labels_num 是 3，模型最后就会输出 3 个数字（例如 [0.9, 0.8, 0.1]）。</span></span><br><span class=\"line\">   <span class=\"keyword\">with</span> warnings.catch_warnings():</span><br><span class=\"line\">       warnings.simplefilter(<span class=\"string\">&#x27;ignore&#x27;</span>)</span><br><span class=\"line\">       train_y = mlb.transform(train_go).astype(np.float32)</span><br><span class=\"line\">       valid_y = mlb.transform(valid_go).astype(np.float32)</span><br><span class=\"line\">       test_y  = mlb.transform(test_go).astype(np.float32)</span><br><span class=\"line\">   <span class=\"comment\">#将人类可读的 GO Term 列表转换为机器可读的 0/1 矩阵（多热编码向量）。</span></span><br></pre></td></tr></table></figure>\n\n<p>对我们已知的功能进行整合，并且在后续进行测试和验证的时候，只对我们现在已经知道的这些个功能来进行预测。</p>\n<p>在后续的测试和验证阶段：</p>\n<ul>\n<li><strong>只能选已知的</strong>：模型只能从这 1000 个已知功能里挑出它认为概率高的。</li>\n<li><strong>未知的被无视</strong>：如果测试集里的某个蛋白质拥有一个<strong>全新的功能</strong>（从未在训练集中出现过），模型是<strong>绝对无法</strong>预测出这个功能的。<ul>\n<li>因为它压根没有对应的输出神经元来代表这个新功能。</li>\n<li>在计算准确率时，这个新功能通常也会被忽略或导致召回率（Recall）降低，因为它变成了“模型看不见的真实标签”。</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251207172545733.png\" alt=\"image-20251207172545733\"></p>\n<h2 id=\"test-model\"><a href=\"#test-model\" class=\"headerlink\" title=\"test_model\"></a>test_model</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test_model = model(labels_num = labels_num, </span><br><span class=\"line\">                   input_size = network_x.shape[<span class=\"number\">1</span>], </span><br><span class=\"line\">                   embedding_size = model_cnf[<span class=\"string\">&#x27;model&#x27;</span>][<span class=\"string\">&#x27;hidden_size&#x27;</span>], </span><br><span class=\"line\">                   hidden_size = model_cnf[<span class=\"string\">&#x27;model&#x27;</span>][<span class=\"string\">&#x27;hidden_size&#x27;</span>], </span><br><span class=\"line\">                   num_gcn = model_cnf[<span class=\"string\">&#x27;model&#x27;</span>][<span class=\"string\">&#x27;num_gcn&#x27;</span>], </span><br><span class=\"line\">                   dropout = <span class=\"number\">0.5</span>, </span><br><span class=\"line\">                   residual=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<p>这段代码调用了 <code>models_multi.py</code> 中的 <code>model</code> 类，创建了一个具体的神经网络对象。</p>\n<ul>\n<li><p><strong><code>labels_num</code><strong>：</strong>输出层维度</strong>。也就是我们之前算出来的 GO 功能总数（比如 1000）。模型最后会输出 1000 个概率值。</p>\n</li>\n<li><p><strong><code>input_size</code><strong>：</strong>输入特征维度</strong>。即 <code>network_x</code> 的列数（39227），对应 InterPro 的稀疏特征总数。</p>\n</li>\n<li><p>**<code>embedding_size</code> &amp; <code>hidden_size</code>**：通常设为 512。</p>\n<ul>\n<li><code>embedding_size</code>：将那 39227 维的稀疏输入压缩成的稠密向量长度。</li>\n<li><code>hidden_size</code>：GCN 内部处理特征时的向量长度。</li>\n</ul>\n</li>\n<li><p>**<code>num_gcn</code>**：GCN 的层数（默认为 2）。这意味着模型会聚合“邻居的邻居”（2 跳）的信息。</p>\n</li>\n<li><p><strong><code>residual=True</code><strong>：开启</strong>残差连接</strong>。这对应了我们之前讨论的自环边处理，确保在每一层 GCN 计算后，原始特征能直接加到结果上，防止信息丢失。</p>\n</li>\n</ul>\n<h2 id=\"other\"><a href=\"#other\" class=\"headerlink\" title=\"other\"></a>other</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss_fn = nn.BCEWithLogitsLoss()</span><br><span class=\"line\">    <span class=\"comment\">#Binary Cross Entropy With Logits Loss（带 Logits 的二元交叉熵损失）。</span></span><br><span class=\"line\">    <span class=\"comment\">#是 PyTorch 中用于 多标签分类（Multi-label Classification） 或 二分类 任务的标准损失函数。</span></span><br><span class=\"line\">    <span class=\"comment\">#这个函数其实是将两个步骤合二为一了：</span></span><br><span class=\"line\">    <span class=\"comment\">#Sigmoid 激活层：模型的直接输出（称为 Logits）通常是无界的实数</span></span><br><span class=\"line\">    <span class=\"comment\">#Sigmoid 函数把这些数压缩到 $(0, 1)$ 之间，变成“概率值”。</span></span><br><span class=\"line\">    <span class=\"comment\">#BCELoss (二元交叉熵损失)：计算预测概率与真实标签（0或1）之间的差距。</span></span><br><span class=\"line\">    <span class=\"comment\">#合二为一是因为当 Logits 非常大或非常小时（比如 100 或 -100），经过 Sigmoid 再取 Log 容易发生数值溢出（NaN）。</span></span><br><span class=\"line\">    <span class=\"comment\">#BCEWithLogitsLoss 在内部使用了 Log-Sum-Exp 的数学技巧，避免了显式的 Log 操作，从而保证计算非常稳定，不会报错。</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">#在本项目中。对于 1000 个 GO 功能，模型会输出 1000 个数值。</span></span><br><span class=\"line\">    <span class=\"comment\">#这个损失函数会独立地看待每一个功能，把它当作一个“是/否”的二分类问题来算 Loss，最后把所有 Loss 加起来。</span></span><br><span class=\"line\"></span><br><span class=\"line\">    sampler = dgl.dataloading.MultiLayerFullNeighborSampler(model_cnf[<span class=\"string\">&#x27;model&#x27;</span>][<span class=\"string\">&#x27;num_gcn&#x27;</span>])</span><br><span class=\"line\">    <span class=\"comment\">#多层全邻居采样器，把大图切成小块来训练。</span></span><br><span class=\"line\">    <span class=\"comment\">#num_gcn：层数。如果设为 2，当你需要计算节点 A 的特征时，</span></span><br><span class=\"line\">    <span class=\"comment\"># 采样器会抓取 A 的所有邻居（第 1 层），以及这些邻居的所有邻居（第 2 层）。这就是 GCN 的感受域。</span></span><br><span class=\"line\">    <span class=\"comment\">#Full (全)：这是关键点。有些采样器为了省内存，只会随机选 5 个或 10 个邻居</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    used_model_performance = np.array([<span class=\"number\">0.0</span>]*model_cnf[<span class=\"string\">&#x27;train&#x27;</span>][<span class=\"string\">&#x27;ensemble_num&#x27;</span>])</span><br><span class=\"line\">    <span class=\"comment\">#Top-K 保存策略的记录板</span></span><br><span class=\"line\">    <span class=\"comment\">#初始化一个全为 0.0 的数组，长度为 ensemble_num（默认为 3）。</span></span><br><span class=\"line\">    <span class=\"comment\">#在训练过程中，每次验证集跑出新的 AUPR 分数，</span></span><br><span class=\"line\">    <span class=\"comment\"># 程序就会看这个分数能不能挤进这个数组。如果能，就替换掉数组里最小的那个数，并保存模型。</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"ppi-train-idx\"><a href=\"#ppi-train-idx\" class=\"headerlink\" title=\"ppi_train_idx\"></a>ppi_train_idx</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 创建一个巨大的数组，长度等于图中所有节点的数量 (39227+)，默认填 -1</span></span><br><span class=\"line\">ppi_train_idx = np.full(network_x.shape[<span class=\"number\">0</span>], -<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 建立映射关系</span></span><br><span class=\"line\"><span class=\"comment\"># train_ppi 里面存的是：训练集蛋白在图中的【节点 ID】（比如：102, 5005, 3...）</span></span><br><span class=\"line\"><span class=\"comment\"># np.arange(...) 生成的是：这些蛋白在特征列表中的【序号】（0, 1, 2...）</span></span><br><span class=\"line\">ppi_train_idx[train_ppi] = np.arange(train_ppi.shape[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<p>在训练循环中，数据流是这样的：</p>\n<ol>\n<li><strong>DGL DataLoader</strong> 吐出来的是 <strong>全局图节点 ID</strong>（比如它告诉你：这个 Batch 包含节点 <code>1024</code>）。</li>\n<li>但是，你的 <strong>标签数据 (<code>train_y</code>)</strong> 和 <strong>序列特征 (<code>train_esm</code>)</strong> 是单独存放在列表里的，它们的下标是 <code>0, 1, 2...</code>，而不是 <code>1024</code>。</li>\n<li><strong>冲突</strong>：你不能直接用 <code>train_y[1024]</code> 去取标签，因为 <code>train_y</code> 可能只有 5000 行，直接取 <code>1024</code> 可能会取错行，甚至越界（如果 ID 是 99999）。</li>\n</ol>\n<p><strong><code>ppi_train_idx</code> 就是一个“翻译官”</strong>：</p>\n<ul>\n<li><strong>它的下标</strong>：代表 <strong>全局图节点 ID</strong>。</li>\n<li><strong>它的值</strong>：代表该节点在 <strong><code>train_y</code> &#x2F; <code>train_esm</code> 列表中的位置（Index）</strong>。</li>\n</ul>\n<h3 id=\"举个具体的例子\"><a href=\"#举个具体的例子\" class=\"headerlink\" title=\"举个具体的例子\"></a>举个具体的例子</h3><p>假设训练集只有 3 个蛋白质：</p>\n<ul>\n<li><strong>训练列表</strong>：<code>[蛋白A, 蛋白B, 蛋白C]</code></li>\n<li>**对应的图节点 ID (<code>train_ppi</code>)**：<code>[5, 100, 20]</code> （蛋白A是图里的第5号节点，B是100号…）</li>\n<li>**对应的标签 (<code>train_y</code>)**：<ul>\n<li>Index 0: 蛋白A 的标签</li>\n<li>Index 1: 蛋白B 的标签</li>\n<li>Index 2: 蛋白C 的标签</li>\n</ul>\n</li>\n</ul>\n<p><strong>执行代码后 <code>ppi_train_idx</code> 的状态</strong>： 这是一个很长的数组，大部分是 -1，但有几个位置变了：</p>\n<ul>\n<li><code>ppi_train_idx[5]</code> &#x3D; <strong>0</strong></li>\n<li><code>ppi_train_idx[20]</code> &#x3D; <strong>2</strong></li>\n<li><code>ppi_train_idx[100]</code> &#x3D; <strong>1</strong></li>\n</ul>\n<p><strong>后续使用时（在 Batch 循环里）</strong>： DGL 给你吐出了一个节点 ID <code>100</code>。 你想找它的标签：</p>\n<ol>\n<li><p>查表：<code>real_index = ppi_train_idx[100]</code> -&gt; 得到 <strong>1</strong>。</p>\n</li>\n<li><p>取值：<code>label = train_y[1]</code> -&gt; <strong>成功拿到蛋白B的标签</strong>。</p>\n</li>\n</ol>\n<h2 id=\"dataloader\"><a href=\"#dataloader\" class=\"headerlink\" title=\"dataloader\"></a>dataloader</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> input_nodes, output_nodes, blocks <span class=\"keyword\">in</span> tqdm(dataloader, leave=<span class=\"literal\">False</span>, desc=<span class=\"string\">&#x27;Training Epoch &#123;&#125;: &#x27;</span>.<span class=\"built_in\">format</span>(e)):</span><br><span class=\"line\"><span class=\"comment\">#dataloader的作用就是打包和输送，每次从训练集随机挑选蛋白质，按照采样器 (sampler) 的规则，向外挖掘两层邻居</span></span><br><span class=\"line\"><span class=\"comment\">#将这些核心蛋白、挖出来的邻居、以及它们之间的连线，打包成一个计算专用的子图结构（即 blocks）。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#这个循环是从传送带（dataloader）源源不断的拿数据包，tqdm 是用来显示进度条的。</span></span><br><span class=\"line\"><span class=\"comment\">#数据包被拆成了三样东西，output_nodes：本次 Batch 要负责预测的那 40 个蛋白质。</span></span><br><span class=\"line\"><span class=\"comment\">#作用：只有对它们的预测结果会用来计算 Loss（算分），反向传播更新参数。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#input_nodes：包含了output_nodes+它们的所有邻居+邻居的邻居。</span></span><br><span class=\"line\"><span class=\"comment\">#作用：需要去特征矩阵（network_x）里，把这些节点的特征全部提取出来。因为要计算output_nodes的特征，必须依赖所有参与者的信息。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#包含了每一层 GCN 的连接结构。</span></span><br><span class=\"line\"><span class=\"comment\">#blocks[0]：描述信息如何从“邻居的邻居”传递给“邻居”。</span></span><br><span class=\"line\"><span class=\"comment\">#blocks[1]：描述信息如何从“邻居”汇聚给“核心目标 (output_nodes)”</span></span><br><span class=\"line\">  </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"indptr\"><a href=\"#indptr\" class=\"headerlink\" title=\"indptr\"></a>indptr</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input_features = (torch.from_numpy(network_x[input_nodes].indices).to(device).long(), <span class=\"comment\">#功能 ID 列表</span></span><br><span class=\"line\">\t\t\t\ttorch.from_numpy(network_x[input_nodes].indptr).to(device).long(), <span class=\"comment\">#切分点的偏移量</span></span><br><span class=\"line\">\t\t\t\ttorch.from_numpy(network_x[input_nodes].data).to(device).<span class=\"built_in\">float</span>())<span class=\"comment\">#具体的值</span></span><br></pre></td></tr></table></figure>\n\n<p><code>indptr</code>（index pointer，索引指针）是稀疏矩阵里最反直觉、最难理解的一个概念。我们可以把 <code>indptr</code> 想象成一把<strong>“剪刀”</strong>或者书本的<strong>“目录”</strong>。它的唯一作用就是告诉计算机：<strong>那个长长的 <code>indices</code> 列表，应该在哪里“剪断”，分给每一行。</strong></p>\n<hr>\n<ol>\n<li>准备素材：一个具体的矩阵</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th></th>\n<th><strong>0 列</strong></th>\n<th><strong>1 列</strong></th>\n<th><strong>2 列</strong></th>\n<th><strong>3 列</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>第 0 行</strong></td>\n<td><strong>1</strong></td>\n<td>0</td>\n<td><strong>1</strong></td>\n<td>0</td>\n</tr>\n<tr>\n<td><strong>第 1 行</strong></td>\n<td>0</td>\n<td><strong>1</strong></td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td><strong>第 2 行</strong></td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td><strong>1</strong></td>\n</tr>\n</tbody></table>\n<ol start=\"2\">\n<li>第一步：把所有非零元素“排成一列” (<code>indices</code>)</li>\n</ol>\n<p>CSR 格式为了省空间，先把每一行里的非零元素（按列号）拿出来，强行拼成<strong>一根长绳子</strong>。</p>\n<ul>\n<li><strong>第 0 行</strong> 有非零元素在：列 <strong>0</strong>，列 <strong>2</strong></li>\n<li><strong>第 1 行</strong> 有非零元素在：列 <strong>1</strong></li>\n<li><strong>第 2 行</strong> 有非零元素在：列 <strong>3</strong></li>\n</ul>\n<p>把它们首尾相连，这就是 indices 数组：</p>\n<p>indices &#x3D; [0, 2, 1, 3]</p>\n<p><strong>问题来了：</strong> 计算机只看到了 <code>[0, 2, 1, 3]</code> 这一串数字。它怎么知道哪几个属于第 0 行？哪几个属于第 1 行？</p>\n<ol start=\"3\">\n<li>第二步：<code>indptr</code> 出场（在那根绳子上做记号）</li>\n</ol>\n<p><code>indptr</code> 就是用来划分这根绳子的。它的规则非常简单：<strong>它是累积计数的</strong>。</p>\n<ul>\n<li><strong><code>indptr[0]</code> 永远是 0</strong>。（表示从绳子的第 0 个位置开始算第 0 行）。</li>\n</ul>\n<p>看第 0 行：</p>\n<ul>\n<li>第 0 行有 <strong>2</strong> 个非零元素（列0和列2）。</li>\n<li>所以，第 0 行在绳子上占据的位置是：从 0 开始，往后数 2 个。</li>\n<li>起始位置：<strong>0</strong></li>\n<li>终止位置：0 + 2 &#x3D; <strong>2</strong></li>\n<li><strong><code>indptr</code> 目前记录：<code>[0, 2]</code></strong></li>\n</ul>\n<p>看第 1 行：</p>\n<ul>\n<li>第 1 行有 <strong>1</strong> 个非零元素（列1）。</li>\n<li>紧接着第 0 行后面，起始位置就是第 0 行的终止位置。</li>\n<li>起始位置：<strong>2</strong></li>\n<li>终止位置：2 + 1 &#x3D; <strong>3</strong></li>\n<li><strong><code>indptr</code> 目前记录：<code>[0, 2, 3]</code></strong></li>\n</ul>\n<p>看第 2 行：</p>\n<ul>\n<li>第 2 行有 <strong>1</strong> 个非零元素（列3）。</li>\n<li>起始位置：<strong>3</strong></li>\n<li>终止位置：3 + 1 &#x3D; <strong>4</strong></li>\n<li><strong><code>indptr</code> 最终记录：<code>[0, 2, 3, 4]</code></strong></li>\n</ul>\n<hr>\n<ol start=\"4\">\n<li>总结：如何用 <code>indptr</code> 读数据？</li>\n</ol>\n<p>现在你拿到了 <code>indptr = [0, 2, 3, 4]</code> 和 <code>indices = [0, 2, 1, 3]</code>。</p>\n<p>计算机想知道 <strong>“第 i 行有哪些数据？”</strong>，它会执行以下逻辑：</p>\n<ul>\n<li><strong>起点 (Start)</strong> &#x3D; <code>indptr[i]</code></li>\n<li><strong>终点 (End)</strong> &#x3D; <code>indptr[i+1]</code></li>\n<li><strong>切片</strong> &#x3D; <code>indices[Start : End]</code> （包含起点，不包含终点）</li>\n</ul>\n<p><strong>套入我们的例子：</strong></p>\n<ol>\n<li><strong>取第 0 行</strong>：<ul>\n<li>查 <code>indptr[0]</code> 是 <strong>0</strong> (起点)。</li>\n<li>查 <code>indptr[1]</code> 是 <strong>2</strong> (终点)。</li>\n<li>去 <code>indices</code> 里切片 <code>indices[0:2]</code> -&gt; 拿到 **<code>[0, 2]</code>**。</li>\n<li><strong>含义</strong>：第 0 行在第 0 列和第 2 列有值。✅</li>\n</ul>\n</li>\n<li><strong>取第 1 行</strong>：<ul>\n<li>查 <code>indptr[1]</code> 是 <strong>2</strong> (起点)。</li>\n<li>查 <code>indptr[2]</code> 是 <strong>3</strong> (终点)。</li>\n<li>去 <code>indices</code> 里切片 <code>indices[2:3]</code> -&gt; 拿到 **<code>[1]</code>**。</li>\n<li><strong>含义</strong>：第 1 行在第 1 列有值。✅</li>\n</ul>\n</li>\n<li><strong>取第 2 行</strong>：<ul>\n<li>查 <code>indptr[2]</code> 是 <strong>3</strong> (起点)。</li>\n<li>查 <code>indptr[3]</code> 是 <strong>4</strong> (终点)。</li>\n<li>去 <code>indices</code> 里切片 <code>indices[3:4]</code> -&gt; 拿到 **<code>[3]</code>**。</li>\n<li><strong>含义</strong>：第 2 行在第 3 列有值。✅</li>\n</ul>\n</li>\n</ol>\n<p>一句话总结</p>\n<p><code>indptr</code> 里的数字，就像是<strong>里程碑</strong>。</p>\n<ul>\n<li><code>0</code> 是起点。</li>\n<li><code>2</code> 意味着：“前 2 个数字属于第 0 行，<strong>从第 2 个数字开始</strong>属于下一行”。</li>\n<li><code>3</code> 意味着：“再往后的 1 个数字属于第 1 行，<strong>从第 3 个数字开始</strong>属于下一行”。</li>\n<li><code>4</code> 意味着：“再往后的 1 个数字属于第 2 行，结束”。</li>\n</ul>\n<p>这就是为什么 <code>indptr</code> 的长度总是比行数多 1（因为要包含最后的结束位置）。</p>\n<h2 id=\"完整的训练逻辑\"><a href=\"#完整的训练逻辑\" class=\"headerlink\" title=\"完整的训练逻辑\"></a>完整的训练逻辑</h2><p><strong>1.外层循环</strong>：拿到一批蛋白质（比如 ID 为 1, 5, 8…）以及它们在 <strong>PPI 网络</strong> 里的邻居结构 (<code>blocks</code>)。</p>\n<p><strong>2.中间准备</strong>：根据这批 ID (1, 5, 8…)，去 <strong>EggNOG 网络</strong> 里准备采样器。</p>\n<p><strong>3.内层循环</strong>：</p>\n<ul>\n<li><strong>取数据</strong>：拿到这批 ID 在 EggNOG 网络里的邻居结构 (<code>egg_blocks</code>)。</li>\n<li><strong>合体</strong>：此时手头既有 PPI 结构，又有 EggNOG 结构。</li>\n<li><strong>训练</strong>：<code>model(egg_blocks, ..., blocks, ...)</code> -&gt; 算出 Loss -&gt; 反向传播。</li>\n</ul>\n<h2 id=\"验证和测试的异同\"><a href=\"#验证和测试的异同\" class=\"headerlink\" title=\"验证和测试的异同\"></a>验证和测试的异同</h2><p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251207164618677.png\" alt=\"image-20251207164618677\"></p>\n<h2 id=\"训练和测试的异同\"><a href=\"#训练和测试的异同\" class=\"headerlink\" title=\"训练和测试的异同\"></a>训练和测试的异同</h2><p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251207170133663.png\" alt=\"image-20251207170133663\"></p>\n","feature":true,"text":"明确概念及用途net_pid_list：读取网络中所有蛋白质 ID 。（如 P12345）。 net_pid_map：ppi网络，也是将蛋白质 ID 翻译成图网...","permalink":"/post/DFMB-main_multi","photos":[],"count_time":{"symbolsCount":"8.3k","symbolsTime":"8 mins."},"categories":[{"name":"DeepFMB","slug":"DeepFMB","count":8,"path":"api/categories/DeepFMB.json"}],"tags":[{"name":"项目","slug":"项目","count":8,"path":"api/tags/项目.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%98%8E%E7%A1%AE%E6%A6%82%E5%BF%B5%E5%8F%8A%E7%94%A8%E9%80%94\"><span class=\"toc-text\">明确概念及用途</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8A%A0%E8%BD%BD%E5%8F%8C%E5%9B%BE%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">加载双图结构</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%87%AA%E7%8E%AF%E8%BE%B9%E7%94%A8%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5\"><span class=\"toc-text\">自环边用来实现残差连接</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%97%AD%E9%9B%86%E5%81%87%E8%AE%BE%E5%92%8C%E6%A0%87%E7%AD%BE%E4%BA%8C%E5%80%BC%E5%8C%96\"><span class=\"toc-text\">闭集假设和标签二值化</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#test-model\"><span class=\"toc-text\">test_model</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#other\"><span class=\"toc-text\">other</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#ppi-train-idx\"><span class=\"toc-text\">ppi_train_idx</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%B8%BE%E4%B8%AA%E5%85%B7%E4%BD%93%E7%9A%84%E4%BE%8B%E5%AD%90\"><span class=\"toc-text\">举个具体的例子</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#dataloader\"><span class=\"toc-text\">dataloader</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#indptr\"><span class=\"toc-text\">indptr</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%AE%8C%E6%95%B4%E7%9A%84%E8%AE%AD%E7%BB%83%E9%80%BB%E8%BE%91\"><span class=\"toc-text\">完整的训练逻辑</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%AA%8C%E8%AF%81%E5%92%8C%E6%B5%8B%E8%AF%95%E7%9A%84%E5%BC%82%E5%90%8C\"><span class=\"toc-text\">验证和测试的异同</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E7%9A%84%E5%BC%82%E5%90%8C\"><span class=\"toc-text\">训练和测试的异同</span></a></li></ol>","author":{"name":"Qushubiao","slug":"blog-author","avatar":"https://s2.loli.net/2025/10/18/RHJMI8AE6TVS21l.jpg","link":"/","description":"做难事必有所得","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"DFMB-复现成功","uid":"eb99b7943b81c4ad9ea239fcf5489374","slug":"DFMB-复现成功","date":"2025-11-23T16:00:00.000Z","updated":"2025-11-23T13:13:41.573Z","comments":true,"path":"api/articles/DFMB-复现成功.json","keywords":null,"cover":[],"text":"​ 今天是2025年11月23日，经过两个多星期的挣扎，终于是弄好了项目的环境和数据集。前面一周一直在找各种依赖，还不知道这个项目是在哪种服务器上跑的，从实验室...","permalink":"/post/DFMB-复现成功","photos":[],"count_time":{"symbolsCount":367,"symbolsTime":"1 mins."},"categories":[{"name":"DeepFMB","slug":"DeepFMB","count":8,"path":"api/categories/DeepFMB.json"}],"tags":[{"name":"项目","slug":"项目","count":8,"path":"api/tags/项目.json"}],"author":{"name":"Qushubiao","slug":"blog-author","avatar":"https://s2.loli.net/2025/10/18/RHJMI8AE6TVS21l.jpg","link":"/","description":"做难事必有所得","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}