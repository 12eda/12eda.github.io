{"title":"DFMB-models_multi","uid":"6e88da55ded41c637f4387c33cd85bc0","slug":"DFMB-models_multi","date":"2025-11-12T16:00:00.000Z","updated":"2025-11-12T12:52:08.942Z","comments":true,"path":"api/articles/DFMB-models_multi.json","keywords":null,"cover":[],"content":"<h1 id=\"CustomGraphConv类\"><a href=\"#CustomGraphConv类\" class=\"headerlink\" title=\"CustomGraphConv类\"></a>CustomGraphConv类</h1><p>CustomGraphConv是DeepFMB中<strong>专门为蛋白质网络设计的图卷积层</strong>，负责从蛋白质相互作用网络和同源网络中提取和传播功能信息。</p>\n<p><strong>局部信息聚合</strong> - 从直接邻居学习<br><strong>多层信息传播</strong> - 捕获远距离网络关系<br><strong>自身特征保护</strong> - 防止重要信息丢失<br><strong>端到端训练</strong> - 与整个模型协同优化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CustomGraphConv</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_feats, out_feats, dropout, residual=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(CustomGraphConv, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Linear(in_feats, out_feats)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.residual = residual</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, block, h</span>):</span><br><span class=\"line\">        <span class=\"keyword\">with</span> block.local_scope():<span class=\"comment\"># 开辟临时计算空间</span></span><br><span class=\"line\">            block.srcdata[<span class=\"string\">&#x27;h&#x27;</span>] = h <span class=\"comment\">#将输入特征h赋值给当前图块（block）中所有源节点的特征，键为&#x27;h&#x27;</span></span><br><span class=\"line\">            <span class=\"comment\">#srcdata指的是在消息传递中，作为消息发送方的节点（即边起点的节点）的特征。</span></span><br><span class=\"line\">            <span class=\"comment\">#dstdata指的是作为消息接收方的节点（即边终点的节点）的特征。</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">#从邻居蛋白质收集特征信息,这些都是张量求和</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.residual:<span class=\"comment\">#如果使用残差连接，则要保留蛋白质的原始特征</span></span><br><span class=\"line\">                block.update_all(fn.u_mul_e(<span class=\"string\">&#x27;h&#x27;</span>, <span class=\"string\">&#x27;self&#x27;</span>, <span class=\"string\">&#x27;m_res&#x27;</span>),<span class=\"comment\">#将源节点的特征&#x27;h&#x27;与自环边的特征&#x27;self&#x27;相乘，得到消息&#x27;m_res&#x27;。</span></span><br><span class=\"line\">                                 fn.<span class=\"built_in\">sum</span>(<span class=\"string\">&#x27;m_res&#x27;</span>, <span class=\"string\">&#x27;res&#x27;</span>))<span class=\"comment\">#是聚合函数，表示对每个目标节点，将来自所有入边的消息&#x27;m_res&#x27;求和，并将结果存储在目标节点的特征&#x27;res&#x27;中。</span></span><br><span class=\"line\">                                <span class=\"comment\">#每个节点的&#x27;res&#x27;就是它自己的特征。这个操作是为了残差连接准备的，保留节点自身的特征。</span></span><br><span class=\"line\"></span><br><span class=\"line\">            block.update_all(fn.u_mul_e(<span class=\"string\">&#x27;h&#x27;</span>, <span class=\"string\">&#x27;ppi&#x27;</span>, <span class=\"string\">&#x27;ppi_m_out&#x27;</span>), <span class=\"comment\"># 这是一个内置的消息函数，表示对于每一条边，将源节点的特征&#x27;h&#x27;与边的特征&#x27;ppi&#x27;相乘，得到消息&#x27;ppi_m_out&#x27;。</span></span><br><span class=\"line\">                             fn.<span class=\"built_in\">sum</span>(<span class=\"string\">&#x27;ppi_m_out&#x27;</span>, <span class=\"string\">&#x27;ppi_out&#x27;</span>))<span class=\"comment\">#是聚合函数，表示对每个目标节点，将来自所有入边的消息&#x27;ppi_m_out&#x27;求和，并将结果存储在目标节点的特征&#x27;ppi_out&#x27;中。</span></span><br><span class=\"line\">                                <span class=\"comment\">#PPI边的消息聚合得到来自邻居的特征。</span></span><br><span class=\"line\"></span><br><span class=\"line\">            h_dst = <span class=\"variable language_\">self</span>.dropout(F.relu(<span class=\"variable language_\">self</span>.W(block.dstdata[<span class=\"string\">&#x27;ppi_out&#x27;</span>])))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.residual:</span><br><span class=\"line\">                h_dst = h_dst + block.dstdata[<span class=\"string\">&#x27;res&#x27;</span>]</span><br><span class=\"line\">                <span class=\"comment\">#通过残差连接保留每个蛋白质的独特特征</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> h_dst</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset_parameters</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.W.weight)</span><br><span class=\"line\">        <span class=\"comment\">#使用Xavier均匀初始化来初始化线性层self.W的权重。</span></span><br><span class=\"line\">        <span class=\"comment\"># 确保了CustomGraphConv中的线性变换层从一个合理的权重分布开始训练</span></span><br><span class=\"line\">        <span class=\"comment\"># 这样做的目的是为了帮助模型更快地收敛，并提高训练稳定性。</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"srcdata-详解\"><a href=\"#srcdata-详解\" class=\"headerlink\" title=\"srcdata 详解\"></a><code>srcdata</code> 详解</h2><p><code>srcdata</code> 是 <strong>DGL（Deep Graph Library）框架中的一个核心概念</strong>，专门用于图神经网络的消息传递。<code>srcdata</code> 是 <strong>源节点特征数据容器</strong>，存储图中所有<strong>消息发送方</strong>节点的特征数据。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">block.srcdata    <span class=\"comment\"># 源节点特征 (消息发送方)</span></span><br><span class=\"line\">block.dstdata    <span class=\"comment\"># 目标节点特征 (消息接收方)  </span></span><br><span class=\"line\">block.edata      <span class=\"comment\"># 边特征 (连接权重等)</span></span><br></pre></td></tr></table></figure>\n\n<p>在消息传递中，我们通常是从源节点发送消息，然后目标节点接收消息。所以，我们需要将特征赋值给源节点，这样在消息函数中就可以使用这些特征来计算消息。在<code>CustomGraphConv</code>中，我们使用<code>block</code>来进行消息传递。我们首先将输入特征<code>h</code>赋值给源节点，然后通过<code>update_all</code>函数进行消息传递和聚合。</p>\n<h2 id=\"残差连接\"><a href=\"#残差连接\" class=\"headerlink\" title=\"残差连接\"></a>残差连接</h2><p><strong>残差连接是深度学习中的一种技术，最初在ResNet中提出，用于解决深层网络训练时的梯度消失和网络退化问题。其核心思想是通过将前一层的输出直接与后一层的输出相加，使得网络可以更容易地学习恒等映射，从而让网络更深。</strong></p>\n<p>在CustomGraphConv中，残差连接的具体实现如下：</p>\n<ol>\n<li><p>在forward函数中，首先判断是否使用残差连接（if self.residual）。</p>\n</li>\n<li><p>如果使用，则先计算自环边的消息传递，即每个节点通过自环边将自己的特征乘以1（自环边的权重为1）然后传递给自己，这样得到的结果就是节点自身的特征，然后通过fn.sum将消息聚合（实际上就是自身特征，因为自环边只有一条）存储到block.dstdata[‘res’]中。</p>\n</li>\n<li><p>然后进行正常的PPI网络的消息传递，将源节点的特征乘以边的权重（ppi）得到消息，然后通过fn.sum聚合到目标节点，得到block.dstdata[‘ppi_out’]。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112190657213.png\" alt=\"image-20251112190657213\"></p>\n</li>\n<li><p>接着，对聚合后的特征进行线性变换、激活函数和dropout，得到h_dst。</p>\n</li>\n<li><p>如果使用残差连接，则将h_dst加上之前保存的自身特征（block.dstdata[‘res’]）。</p>\n</li>\n</ol>\n<p>这样，残差连接就实现了将输入（即节点自身的特征）直接加到了经过一层图卷积变换后的输出上。</p>\n<p>使用残差连接时，我们保存了节点自身的特征（备份），然后在输出时将原始特征加上去。这样，即使网络学习到的变换（h_dst）不理想，至少保留了原始特征，使得网络不会比原始特征更差。这有助于梯度反向传播，并且可以让网络更深的层数得以训练。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112154916640.png\" alt=\"image-20251112154916640\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112154931046.png\" alt=\"image-20251112154931046\"></p>\n<h2 id=\"Dropout如何防止过拟合\"><a href=\"#Dropout如何防止过拟合\" class=\"headerlink\" title=\"Dropout如何防止过拟合\"></a>Dropout如何防止过拟合</h2><h3 id=\"Dropout的基本思想：\"><a href=\"#Dropout的基本思想：\" class=\"headerlink\" title=\"Dropout的基本思想：\"></a>Dropout的基本思想：</h3><p><strong>在训练时随机”关闭”一部分神经元，强迫网络不依赖任何特定的神经元</strong></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112160052144.png\" alt=\"image-20251112160052144\"></p>\n<h1 id=\"Attention类\"><a href=\"#Attention类\" class=\"headerlink\" title=\"Attention类\"></a>Attention类</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 为每个蛋白质自动学习两种网络特征的相对重要性</span></span><br><span class=\"line\">输入: [PPI特征, 同源特征]  <span class=\"comment\"># 两个512维特征</span></span><br><span class=\"line\">输出: 融合特征 + 注意力权重</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 不是简单平均，而是加权融合</span></span><br><span class=\"line\">融合特征 = PPI权重 × PPI特征 + 同源权重 × 同源特征</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 示例权重：</span></span><br><span class=\"line\">蛋白质A: PPI权重=<span class=\"number\">0.8</span>, 同源权重=<span class=\"number\">0.2</span>  <span class=\"comment\"># 主要依赖PPI网络</span></span><br><span class=\"line\">蛋白质B: PPI权重=<span class=\"number\">0.3</span>, 同源权重=<span class=\"number\">0.7</span>  <span class=\"comment\"># 主要依赖同源关系</span></span><br><span class=\"line\">蛋白质C: PPI权重=<span class=\"number\">0.5</span>, 同源权重=<span class=\"number\">0.5</span>  <span class=\"comment\"># 两者均衡</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">三大特征源：</span><br><span class=\"line\">序列特征 (ESM) ───────────────┐</span><br><span class=\"line\">                              ↓ 拼接 → 预测</span><br><span class=\"line\">PPI特征 → Attention → 融合特征 ─┘</span><br><span class=\"line\">              ↑</span><br><span class=\"line\">同源特征 ──────┘</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Attention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_size, hidden_size=<span class=\"number\">16</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Attention, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.project = nn.Sequential(</span><br><span class=\"line\">            nn.Linear(in_size, hidden_size),  <span class=\"comment\"># in_size=75</span></span><br><span class=\"line\">            nn.Tanh(),<span class=\"comment\">#非线性激活</span></span><br><span class=\"line\">            nn.Linear(hidden_size, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 通过beta值的比例关系，可以判断某个蛋白质的ppi特征和同源特征之间的权重关系</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, z</span>):</span><br><span class=\"line\">        w = <span class=\"variable language_\">self</span>.project(z)</span><br><span class=\"line\">        <span class=\"comment\"># z 的形状: [batch_size, 2, in_size]</span></span><br><span class=\"line\">        <span class=\"comment\"># 其中:</span></span><br><span class=\"line\">        <span class=\"comment\"># - batch_size: 批处理大小（一次处理多少个蛋白质）</span></span><br><span class=\"line\">        <span class=\"comment\"># - 2: 两个要融合的特征（PPI特征和同源特征）</span></span><br><span class=\"line\">        <span class=\"comment\"># - in_size: 每个特征的维度（在DeepFMB中是512）</span></span><br><span class=\"line\">        <span class=\"comment\">#经过project这个神经网络之后，w的形状为[batch_size, 2, 1]</span></span><br><span class=\"line\">        beta = F.softmax(w, dim=<span class=\"number\">1</span>)<span class=\"comment\">#确保每个蛋白质的两个特征权重之和为1</span></span><br><span class=\"line\">        <span class=\"comment\">#beta的形状为[batch_size, 2, 1]</span></span><br><span class=\"line\">        <span class=\"comment\"># print(z.size(), w.size(), beta.size())</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (beta * z).<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>), beta<span class=\"comment\">#.sum(1) = 把维度1（z里面的2）&quot;压缩掉&quot;，将两个特征合并成一个</span></span><br><span class=\"line\">        <span class=\"comment\"># 返回两个特征之间的权重和权重乘积</span></span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset_parameters</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.project[<span class=\"number\">0</span>].weight)</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.project[<span class=\"number\">2</span>].weight)</span><br><span class=\"line\">        <span class=\"comment\">#给project的两个线性层初始化权重</span></span><br></pre></td></tr></table></figure>\n\n<p>对于forward函数的详细流程：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112170436836.png\" alt=\"image-20251112170436836\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = <span class=\"variable language_\">self</span>.project(z)</span><br></pre></td></tr></table></figure>\n\n\n\n<ol>\n<li><p><code>z</code> 的形状是 <code>[batch_size, 2, 512]</code></p>\n</li>\n<li><p><code>self.project</code> 是一个神经网络，对每个特征向量进行处理：</p>\n<ul>\n<li>输入：512维特征</li>\n<li><strong>经过：Linear(512→16) → Tanh() → Linear(16→1)</strong></li>\n<li>输出：单个标量值</li>\n</ul>\n</li>\n<li><p>结果 <code>w</code> 的形状是 <code>[batch_size, 2, 1]</code></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112170549164.png\" alt=\"image-20251112170549164\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beta = F.softmax(w, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<ol>\n<li><p>对 <code>w</code> 在维度1（特征维度）上应用softmax</p>\n</li>\n<li><p>确保每个蛋白质的两个特征权重之和为1</p>\n</li>\n<li><p>结果 <code>beta</code> 的形状仍然是 <code>[batch_size, 2, 1]</code></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112170717443.png\" alt=\"image-20251112170717443\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">return</span> (beta * z).<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>), beta</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112171458733.png\" alt=\"image-20251112171458733\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112170757693.png\" alt=\"image-20251112170757693\"><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112170807442.png\" alt=\"image-20251112170807442\"></p>\n</li>\n</ol>\n</li>\n</ol>\n<p>整体计算流程如下：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112170853663.png\" alt=\"image-20251112170853663\"></p>\n<h1 id=\"model类\"><a href=\"#model类\" class=\"headerlink\" title=\"model类\"></a>model类</h1><p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112202612990.png\" alt=\"image-20251112202612990\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">model</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, labels_num, input_size, embedding_size, hidden_size, num_gcn, dropout, residual</span>):</span><br><span class=\"line\">        <span class=\"comment\">#labels_num: GO术语数量（输出维度）</span></span><br><span class=\"line\">        <span class=\"comment\">#input_size: InterPro特征原始维度（39227维）</span></span><br><span class=\"line\">        <span class=\"comment\">#embedding_size: 嵌入层输出维度（将稀疏特征转为稠密向量）</span></span><br><span class=\"line\">        <span class=\"comment\"># #hidden_size: 隐藏层维度（通常是512）</span></span><br><span class=\"line\">        <span class=\"comment\"># #num_gcn: GCN层数（通常是2）</span></span><br><span class=\"line\">        <span class=\"comment\"># #dropout: Dropout比率（防止过拟合）</span></span><br><span class=\"line\">        <span class=\"comment\"># #residual: 是否使用残差连接</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(model, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.labels_num = labels_num</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.embedding_layer = nn.EmbeddingBag(input_size, embedding_size, mode=<span class=\"string\">&#x27;sum&#x27;</span>, include_last_offset=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 嵌入层，目的是压缩稀疏特征，将多个特征聚合到一个向量中。</span></span><br><span class=\"line\">        <span class=\"comment\">#将39227维稀疏二进制特征 → embedding_size维稠密向量。mode=&#x27;sum&#x27;: 对多个特征进行求和聚合。include_last_offset=True: 处理变长序列的技术参数</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"comment\">#随机丢弃神经元，防止过拟合</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.ppi_gcn_layers = nn.ModuleList(CustomGraphConv(embedding_size, hidden_size, dropout=<span class=\"number\">0.5</span>, residual=residual) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_gcn))</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.egg_gcn_layers = nn.ModuleList(CustomGraphConv(embedding_size, hidden_size, dropout=<span class=\"number\">0.5</span>, residual=residual) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_gcn))</span><br><span class=\"line\">        <span class=\"comment\">#使用nn.ModuleList存储多个GCN层，PPI和同源网络使用相同结构但参数独立的GCN，每个GCN层：embedding_size → hidden_size。</span></span><br><span class=\"line\">        <span class=\"comment\">#多层GCN的作用：信息传播的&quot;跳数&quot;，通过多层GCN让每个蛋白质能够获取更远距离的邻居信息，从而学习更丰富的网络上下文特征</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.sequenceLayer=nn.Sequential(</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">1280</span>, <span class=\"number\">2048</span>),<span class=\"comment\"># ESM-1b输出1280维 → 2048维</span></span><br><span class=\"line\">            <span class=\"comment\">#扩张到2048维 → 捕捉更复杂模式</span></span><br><span class=\"line\">            nn.LeakyReLU(),<span class=\"comment\"># 激活函数</span></span><br><span class=\"line\">            nn.Linear(<span class=\"number\">2048</span>, <span class=\"number\">1024</span>), <span class=\"comment\"># 降维</span></span><br><span class=\"line\">            <span class=\"comment\">#压缩到1024维 → 提炼关键信息</span></span><br><span class=\"line\">            nn.Dropout(<span class=\"number\">0.2</span>), <span class=\"comment\"># Dropout</span></span><br><span class=\"line\">            nn.LeakyReLU(),<span class=\"comment\"># 激活函数</span></span><br><span class=\"line\">            nn.Linear(<span class=\"number\">1024</span>, hidden_size),<span class=\"comment\"># 最终降维到hidden_size</span></span><br><span class=\"line\">            <span class=\"comment\">#压缩到512维 → 得到精华特征</span></span><br><span class=\"line\">            nn.Dropout(<span class=\"number\">0.2</span>),  <span class=\"comment\"># Dropout</span></span><br><span class=\"line\">            nn.LeakyReLU(),  <span class=\"comment\"># 激活函数</span></span><br><span class=\"line\">        )</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.attention = Attention(hidden_size, <span class=\"number\">256</span>)<span class=\"comment\"># 注意力融合层，自适应融合PPI和同源特征</span></span><br><span class=\"line\">        <span class=\"comment\">#不同蛋白质自动调整权重，权重告诉我们模型依赖哪种信息，帮助模型学习更适合的表示。</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.trans_layer = nn.Linear(hidden_size*<span class=\"number\">2</span>, hidden_size*<span class=\"number\">2</span>)<span class=\"comment\"># 特征转换层</span></span><br><span class=\"line\">        <span class=\"comment\">#输入特征 = 序列特征(512维) + 融合网络特征(512维)</span></span><br><span class=\"line\">        <span class=\"comment\">#转换后特征 = ReLU(Linear(1024→1024)(输入特征))</span></span><br><span class=\"line\">        <span class=\"comment\">#简单拼接的特征可能不协调，因此需要通过转换层进行协调</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pred_layer = nn.Linear(hidden_size*<span class=\"number\">2</span>, labels_num)<span class=\"comment\"># 最终预测层</span></span><br><span class=\"line\">        <span class=\"comment\">#GO术语数量（输出维度），输出概率预测</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.residual = residual</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.num_gcn = num_gcn</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.reset_parameters()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset_parameters</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.embedding_layer.weight)</span><br><span class=\"line\">        <span class=\"comment\">#初始化InterPro特征嵌入层的权重</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> gcn <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.ppi_gcn_layers:</span><br><span class=\"line\">            gcn.reset_parameters()<span class=\"comment\">#递归调用每个CustomGraphConv层的初始化方法</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> gcn <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.egg_gcn_layers:</span><br><span class=\"line\">            gcn.reset_parameters()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.attention.reset_parameters()</span><br><span class=\"line\">        </span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.trans_layer.weight)</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.pred_layer.weight)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, egg_blocks, egg_input_feature, blocks, input_feature, input_esm</span>):</span><br><span class=\"line\">        <span class=\"comment\">#egg_blocks: 同源网络的DGL图块（多层）</span></span><br><span class=\"line\">        <span class=\"comment\"># #egg_input_feature: 同源网络的InterPro特征</span></span><br><span class=\"line\">        <span class=\"comment\"># #blocks: PPI网络的DGL图块（多层）</span></span><br><span class=\"line\">        <span class=\"comment\"># #input_feature: PPI网络的InterPro特征</span></span><br><span class=\"line\">        <span class=\"comment\"># #input_esm: ESM序列特征张量</span></span><br><span class=\"line\"></span><br><span class=\"line\">        allSequenceFeatures = <span class=\"variable language_\">self</span>.sequenceLayer(input_esm)</span><br><span class=\"line\">        <span class=\"comment\"># 将ESM特征转换为高级序列特征</span></span><br><span class=\"line\">        <span class=\"comment\">#捕捉到更多细节</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        feature_embedding = <span class=\"variable language_\">self</span>.dropout(F.relu(<span class=\"variable language_\">self</span>.embedding_layer(*input_feature)))</span><br><span class=\"line\">        egg_feature_embedding = <span class=\"variable language_\">self</span>.dropout(F.relu(<span class=\"variable language_\">self</span>.embedding_layer(*egg_input_feature)))</span><br><span class=\"line\">        <span class=\"comment\">#将稀疏InterPro特征转换为稠密嵌入向量</span></span><br><span class=\"line\"></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"variable language_\">self</span>.num_gcn):<span class=\"comment\">#通过多层GCN提取PPI网络特征</span></span><br><span class=\"line\">            feature_embedding = <span class=\"variable language_\">self</span>.ppi_gcn_layers[i](blocks[i], feature_embedding)</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"variable language_\">self</span>.num_gcn):<span class=\"comment\">#通过多层GCN提取同源网络特征</span></span><br><span class=\"line\">            egg_feature_embedding = <span class=\"variable language_\">self</span>.egg_gcn_layers[i](egg_blocks[i], egg_feature_embedding)</span><br><span class=\"line\">        <span class=\"comment\">#第1层：使用blocks[0]和初始特征，得到第一层的输出特征。</span></span><br><span class=\"line\">        <span class=\"comment\">#第2层：使用blocks[1]和第一层的输出特征，得到第二层的输出特征。</span></span><br><span class=\"line\">        <span class=\"comment\">#第num_gcn层：使用blocks[num_gcn-1]和前一层输出特征，得到最终的特征。</span></span><br><span class=\"line\">        <span class=\"comment\">#第一层聚合直接邻居（1跳），第二层聚合2跳邻居（因为第一层已经聚合了邻居的特征，第二层再聚合时，相当于聚合了邻居的邻居，即2跳），以此类推。</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        cob_features = torch.stack([feature_embedding, egg_feature_embedding], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\">#创建新维度堆叠，也就是把他俩加一起，堆叠起来。</span></span><br><span class=\"line\"></span><br><span class=\"line\">        cob_features, _ = <span class=\"variable language_\">self</span>.attention(cob_features)</span><br><span class=\"line\">        <span class=\"comment\">#这里 _ 就是 beta，即注意力权重，可忽略</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        all_features = torch.cat((allSequenceFeatures, cob_features), <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\">#拼接序列特征(ESM)和融合的网络特征</span></span><br><span class=\"line\">        <span class=\"comment\">#在现有维度上连接，也就是1+1+2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        all_features = F.relu(<span class=\"variable language_\">self</span>.trans_layer(all_features))</span><br><span class=\"line\">        <span class=\"comment\">#精炼和转换拼接后的特征</span></span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = <span class=\"variable language_\">self</span>.pred_layer(all_features)</span><br><span class=\"line\">        <span class=\"comment\">#预测每个GO术语的概率</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<p>信息整合：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112174955900.png\" alt=\"image-20251112174955900\"><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112175816284.png\" alt=\"image-20251112175816284\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112180804167.png\" alt=\"image-20251112180804167\"></p>\n<p>具体流程：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/12eda/Pictures@main/image-20251112182225995.png\" alt=\"image-20251112182225995\"></p>\n","feature":true,"text":"CustomGraphConv类CustomGraphConv是DeepFMB中专门为蛋白质网络设计的图卷积层，负责从蛋白质相互作用网络和同源网络中提取和传播功...","permalink":"/post/DFMB-models_multi","photos":[],"count_time":{"symbolsCount":"9.7k","symbolsTime":"9 mins."},"categories":[{"name":"DeepFMB","slug":"DeepFMB","count":6,"path":"api/categories/DeepFMB.json"}],"tags":[{"name":"项目","slug":"项目","count":6,"path":"api/tags/项目.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#CustomGraphConv%E7%B1%BB\"><span class=\"toc-text\">CustomGraphConv类</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#srcdata-%E8%AF%A6%E8%A7%A3\"><span class=\"toc-text\">srcdata 详解</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5\"><span class=\"toc-text\">残差连接</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Dropout%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88\"><span class=\"toc-text\">Dropout如何防止过拟合</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Dropout%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%EF%BC%9A\"><span class=\"toc-text\">Dropout的基本思想：</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Attention%E7%B1%BB\"><span class=\"toc-text\">Attention类</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#model%E7%B1%BB\"><span class=\"toc-text\">model类</span></a></li></ol>","author":{"name":"Qushubiao","slug":"blog-author","avatar":"https://s2.loli.net/2025/10/18/RHJMI8AE6TVS21l.jpg","link":"/","description":"做难事必有所得","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"DFMB-data_utils","uid":"b1dc64af12759d9df8f8c0d6049f8b08","slug":"DFMB-data_utils","date":"2025-11-07T16:00:00.000Z","updated":"2025-11-12T12:51:22.019Z","comments":true,"path":"api/articles/DFMB-data_utils.json","keywords":null,"cover":[],"text":"涉及以下函数的实现： 12__all__ = ['get_pid_list', 'get_go_list', 'get_pid_go', 'get_pid_go...","permalink":"/post/DFMB-data_utils","photos":[],"count_time":{"symbolsCount":"6.2k","symbolsTime":"6 mins."},"categories":[{"name":"DeepFMB","slug":"DeepFMB","count":6,"path":"api/categories/DeepFMB.json"}],"tags":[{"name":"项目","slug":"项目","count":6,"path":"api/tags/项目.json"}],"author":{"name":"Qushubiao","slug":"blog-author","avatar":"https://s2.loli.net/2025/10/18/RHJMI8AE6TVS21l.jpg","link":"/","description":"做难事必有所得","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}